# VoiceAI Lab

A **personal, portable MVP** for voice synthesis and conversion tasks. Features both Web (React) and Desktop (PyQt) interfaces sharing a unified FastAPI backend.

## ğŸ¯ Features

- **Text-to-Speech (TTS)**: XTTS and OpenVoice engines with multi-language support
- **Voice Conversion (VC)**: RVC and So-VITS-SVC for voice transformation
- **Speaker Management**: JSON-based speaker profiles with sample audio
- **Batch Processing**: Multi-text/audio processing with progress tracking
- **Dual Interface**: Web UI for accessibility + Desktop app for offline use
- **Model Switching**: Engine swapping via `.env` configuration
- **Audio Optimization**: LUFS normalization and silence trimming

## ğŸ—ï¸ Architecture

```
voice-app/
â”œâ”€â”€ backend/              # FastAPI backend service
â”‚   â”œâ”€â”€ app.py           # Main FastAPI application entry point
â”‚   â”œâ”€â”€ services/        # Core business logic
â”‚   â”‚   â”œâ”€â”€ tts_service.py    # TTS engine wrapper (XTTS/OpenVoice)
â”‚   â”‚   â”œâ”€â”€ vc_service.py     # VC engine wrapper (RVC/So-VITS-SVC)
â”‚   â”‚   â””â”€â”€ audio_utils.py    # Audio processing (LUFS/trim/convert)
â”‚   â”œâ”€â”€ core/            # Core utilities
â”‚   â”‚   â”œâ”€â”€ model_manager.py  # Model loading/unloading/caching
â”‚   â”‚   â””â”€â”€ config.py         # Environment configuration management
â”‚   â””â”€â”€ requirements.txt      # Minimal dependency list
â”‚
â”œâ”€â”€ web/                 # React WebUI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ TTSPage.tsx   # TTS operation interface
â”‚   â”‚   â”‚   â””â”€â”€ VCPage.tsx    # VC operation interface
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioPlayer.tsx
â”‚   â”‚   â”‚   â””â”€â”€ FileUpload.tsx
â”‚   â”‚   â”œâ”€â”€ services/api.ts   # Backend API calls
â”‚   â”‚   â””â”€â”€ App.tsx           # Main application router
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ desktop/             # PyQt desktop application
â”‚   â”œâ”€â”€ main.py          # PyQt main window
â”‚   â”œâ”€â”€ widgets/         # UI components
â”‚   â”‚   â”œâ”€â”€ tts_widget.py     # TTS operation panel
â”‚   â”‚   â””â”€â”€ vc_widget.py      # VC operation panel
â”‚   â””â”€â”€ requirements.txt      # PyQt dependencies
â”‚
â”œâ”€â”€ data/                # Data directory
â”‚   â”œâ”€â”€ speakers/        # Speaker configuration files (JSON)
â”‚   â”œâ”€â”€ outputs/         # Generated audio files
â”‚   â””â”€â”€ cache/           # Model cache for offline use
â”‚
â”œâ”€â”€ models/              # Model files (.gitignore)
â”‚   â”œâ”€â”€ tts/             # TTS models
â”‚   â””â”€â”€ vc/              # VC models
â”‚
â””â”€â”€ scripts/             # Utility scripts
    â”œâ”€â”€ download_models.py    # Model download automation
    â””â”€â”€ test_api.py          # API testing scripts
```

## ğŸš€ Quick Start

### Prerequisites

- NVIDIA GPU with 8-12GB VRAM
- CUDA 12.1+
- Conda/Miniconda

### Installation

```bash
# Create environment
conda create -n audio-speech-lab python=3.10
conda activate audio-speech-lab

# Clone repository
git clone <repository-url>
cd voice-app

# Install backend dependencies
cd backend
pip install -r requirements.txt

# Install web dependencies
cd ../web
npm install

# Install desktop dependencies
cd ../desktop
pip install -r requirements.txt

# Download models
cd ../scripts
python download_models.py
```

### Configuration

Copy `.env.example` to `.env` and configure:

```env
# Device & Performance
DEVICE=cuda
FP16=true
MAX_CONCURRENCY=2

# Engine Selection (restart to switch)
TTS_ENGINE=xtts     # xtts | openvoice | bark
VC_ENGINE=rvc       # rvc | sovits

# Paths
MODELS_DIR=./models
SPEAKER_DIR=./data/speakers
OUTPUT_DIR=./data/outputs
API_PREFIX=/api/v1
ALLOWED_ORIGINS=http://localhost:3000

# Audio Processing
TARGET_LUFS=-16
SAMPLE_RATE=22050
```

### Running the Application

```bash
# Start backend (Terminal 1)
cd backend && python app.py

# Start web UI (Terminal 2)
cd web && npm run dev

# Start desktop app (Terminal 3)
cd desktop && python main.py
```

## ğŸ“¡ API Reference

### Text-to-Speech

**POST** `/api/v1/tts`

```json
{
  "text": "Text to synthesize",
  "speaker_id": "default",
  "language": "en",
  "speed": 1.0
}
```

**Response:**
```json
{
  "audio_url": "/outputs/tts_xxx.wav",
  "duration": 5.2,
  "processing_time": 3.1
}
```

### Voice Conversion

**POST** `/api/v1/vc`

```json
{
  "source_audio": "base64_encoded_wav",
  "target_speaker": "speaker_001",
  "preserve_pitch": true
}
```

**Response:**
```json
{
  "audio_url": "/outputs/vc_xxx.wav",
  "processing_time": 8.5
}
```

### Health Check

**GET** `/health`

Returns system status and available engines.

## ğŸ§ª Testing

```bash
# Health check
curl http://localhost:8000/health

# Test TTS
curl -X POST http://localhost:8000/api/v1/tts \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello World","speaker_id":"default"}' \
  --output test.wav

# Test with script
python scripts/test_api.py
```

## ğŸ›ï¸ Model Management

### Engine Switching

Modify `.env` and restart the backend:

```env
TTS_ENGINE=openvoice  # Switch from XTTS to OpenVoice
VC_ENGINE=sovits      # Switch from RVC to So-VITS-SVC
```

### Speaker Profiles

Speaker configurations are stored in `data/speakers/` as JSON files:

```json
{
  "id": "speaker_001",
  "name": "Alice",
  "language": "en",
  "gender": "female",
  "sample_audio": "alice_sample.wav",
  "description": "Clear female voice"
}
```

## âš¡ Performance Optimization

### Memory Requirements

- **TTS (XTTS)**: ~2.5GB VRAM
- **VC (RVC)**: ~1.5GB VRAM
- **Total**: ~4GB (fits comfortably in 8GB VRAM)

### Recommended Settings

```python
# Automatic optimizations enabled by default
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('medium')

# Audio normalization to -16 LUFS
# FP16 precision for 2x memory savings
# Dynamic model loading/unloading
```

## ğŸ› Troubleshooting

### Common Issues

**CUDA Out of Memory**
- Reduce `MAX_CONCURRENCY` in `.env`
- Enable FP16: `FP16=true`
- Close other GPU applications

**Audio Quality Issues**
- Check `TARGET_LUFS` setting (-16 recommended)
- Verify input audio format (WAV, 22050Hz preferred)
- Use `preserve_pitch=true` for voice conversion

**Model Loading Errors**
- Run `python scripts/download_models.py`
- Check `MODELS_DIR` path in `.env`
- Ensure sufficient disk space (models ~2-5GB each)

## ğŸ”§ Development

### Project Structure Philosophy

- **Minimal Dependencies**: Only essential packages
- **Single Repository**: Mono-repo approach for simplicity
- **Offline-First**: Desktop app works without internet
- **API-Driven**: Shared backend for multiple frontends
- **Configuration-Based**: Engine switching via environment

### Adding New Engines

1. Implement service in `backend/services/`
2. Update `model_manager.py` loading logic
3. Add engine option to `.env.example`
4. Update API documentation

### Git Workflow

```bash
# Feature branches
git checkout -b feature/new-tts-engine

# Conventional commits
git commit -m "feat(tts): add bark engine support"
git commit -m "fix(vc): resolve audio format compatibility"
git commit -m "docs(api): update endpoint documentation"
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

# Ubuntu/Debian
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# CentOS/RHEL
curl -fsSL https://rpm.nodesource.com/setup_18.x | sudo bash -
sudo yum install -y nodejs

# macOS
brew install node

# Windows
winget install OpenJS.NodeJS
```

#### å®‰è£ FFmpeg
```bash
# Ubuntu/Debian
sudo apt update && sudo apt install ffmpeg

# CentOS/RHEL
sudo yum install ffmpeg

# macOS
brew install ffmpeg

# Windows (ä½¿ç”¨ Chocolatey)
choco install ffmpeg
```

### 2. å°ˆæ¡ˆä¸‹è¼‰èˆ‡è¨­å®š

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/your-username/voice-app.git
cd voice-app

# å»ºç«‹ Python ç’°å¢ƒ
conda create -n audio-speech-lab python=3.10
conda activate audio-speech-lab

# è¨­å®šå¿«å–è·¯å¾‘ (é‡è¦!)
export AI_CACHE_ROOT="/path/to/your/ai/cache"
# Windows: set AI_CACHE_ROOT=C:\your\ai\cache
```

### 3. è‡ªå‹•åŒ–å®‰è£

```bash
# ä¸€éµå®‰è£æ‰€æœ‰ä¾è³´
python scripts/setup_dev.py

# æª¢æŸ¥å®‰è£ç‹€æ…‹
python scripts/setup_dev.py --status
```

### 4. æ‰‹å‹•å®‰è£ (å¦‚éœ€è¦)

#### å¾Œç«¯ä¾è³´
```bash
cd backend
pip install -r requirements.txt
```

#### React å‰ç«¯
```bash
cd frontend/react_app
npm install
```

#### Gradio å‰ç«¯
```bash
cd frontend/gradio_app
pip install -r requirements.txt
```

#### PyQt æ¡Œé¢ç‰ˆ
```bash
cd frontend/pyqt_app
pip install -r requirements.txt
```

### 5. æ¨¡å‹è¨­å®š

```bash
# å‰µå»ºæ¨¡å‹ç›®éŒ„èˆ‡ç¯„ä¾‹è¨­å®š
python scripts/download_models.py --setup

# æª¢æŸ¥æ¨¡å‹ç‹€æ…‹
python scripts/download_models.py --check
```

### 6. é…ç½®æª”æ¡ˆ

è¤‡è£½ä¸¦ç·¨è¼¯ç’°å¢ƒè®Šæ•¸ï¼š
```bash
cd backend
cp .env.example .env
nano .env  # ç·¨è¼¯é…ç½®
```

é—œéµé…ç½®é …ï¼š
```env
# GPU è¨­å®š
DEVICE=cuda              # cuda | cpu
FP16=true               # ä½¿ç”¨åŠç²¾åº¦æµ®é»æ•¸

# å¼•æ“é¸æ“‡
TTS_ENGINE=xtts         # xtts | openvoice | bark
VC_ENGINE=rvc           # rvc | sovits

# è·¯å¾‘é…ç½® (ä¿®æ”¹ç‚ºä½ çš„è·¯å¾‘)
AI_CACHE_ROOT=/home/user/ai_cache
OUTPUT_DIR=${AI_CACHE_ROOT}/outputs/voice-app
SPEAKER_DIR=${AI_CACHE_ROOT}/voice/speakers

# ç¶²è·¯è¨­å®š
API_PREFIX=/api/v1
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:7860
MAX_CONCURRENCY=2       # ä¸¦ç™¼è«‹æ±‚æ•¸é‡
```

### 7. é¦–æ¬¡å•Ÿå‹•æ¸¬è©¦

```bash
# å•Ÿå‹•å¾Œç«¯
cd backend
python -m api.main

# æ–°çµ‚ç«¯ï¼šæ¸¬è©¦ API
python scripts/test_api.py --quick

# æˆåŠŸè¨Šæ¯ç¯„ä¾‹:
# âœ… Backend is healthy
# ğŸ“¡ Backend URL: http://localhost:8000
```

## ğŸ”§ å¹³å°ç‰¹å®šè¨­å®š

### Windows è¨­å®š

#### PowerShell åŸ·è¡ŒåŸå‰‡
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

#### ç’°å¢ƒè®Šæ•¸è¨­å®š
```batch
# ç³»çµ±ç’°å¢ƒè®Šæ•¸
setx AI_CACHE_ROOT "C:\Users\%USERNAME%\AppData\Local\VoiceApp\cache"

# æˆ–åœ¨ .env æª”æ¡ˆä¸­è¨­å®š
AI_CACHE_ROOT=C:/Users/%USERNAME%/AppData/Local/VoiceApp/cache
```

#### CUDA å®‰è£
```batch
# å®‰è£ CUDA Toolkit
# ä¸‹è¼‰: https://developer.nvidia.com/cuda-downloads

# é©—è­‰å®‰è£
nvcc --version
python -c "import torch; print(torch.cuda.is_available())"
```

### macOS è¨­å®š

#### Homebrew ä¾è³´
```bash
# å®‰è£é–‹ç™¼å·¥å…·
brew install git python@3.10 node ffmpeg

# M1/M2 Mac ç‰¹æ®Šè¨­å®š
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

#### è·¯å¾‘è¨­å®š
```bash
# åŠ å…¥ .zshrc æˆ– .bashrc
export AI_CACHE_ROOT="$HOME/.cache/voice-app"
export PATH="/opt/homebrew/bin:$PATH"  # M1/M2
```

### Linux è¨­å®š

#### GPU é©…å‹•å®‰è£
```bash
# NVIDIA é©…å‹• (Ubuntu)
sudo apt update
sudo apt install nvidia-driver-525
sudo reboot

# é©—è­‰
nvidia-smi
```

#### æ¬Šé™è¨­å®š
```bash
# éŸ³è¨Šè¨­å‚™æ¬Šé™
sudo usermod -a -G audio $USER

# å¿«å–ç›®éŒ„æ¬Šé™
mkdir -p $AI_CACHE_ROOT
chmod 755 $AI_CACHE_ROOT
```

## ğŸ³ Docker éƒ¨ç½² (é¸ç”¨)

### Dockerfile
```dockerfile
FROM python:3.10-slim

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    && rm -rf /var/lib/apt/lists/*

# è¨­å®šå·¥ä½œç›®éŒ„
WORKDIR /app

# è¤‡è£½ä¸¦å®‰è£ Python ä¾è³´
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å•Ÿå‹•æŒ‡ä»¤
CMD ["python", "-m", "api.main"]
```

### Docker Compose
```yaml
version: '3.8'

services:
  voice-app-backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - AI_CACHE_ROOT=/app/cache
      - DEVICE=cpu
    volumes:
      - ./cache:/app/cache
      - ./data:/app/data

  voice-app-frontend:
    image: node:18-alpine
    working_dir: /app
    command: npm start
    ports:
      - "3000:3000"
    volumes:
      - ./frontend/react_app:/app
    depends_on:
      - voice-app-backend
```

### å®¹å™¨å•Ÿå‹•
```bash
# æ§‹å»ºèˆ‡å•Ÿå‹•
docker-compose up --build

# å¾Œå°é‹è¡Œ
docker-compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f
```

## âš¡ æ•ˆèƒ½èª¿æ ¡

### GPU æœ€ä½³åŒ–

#### CUDA è¨­å®š
```bash
# æª¢æŸ¥ CUDA ç‰ˆæœ¬å…¼å®¹æ€§
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'GPU Count: {torch.cuda.device_count()}')
"
```

#### è¨˜æ†¶é«”ç®¡ç†
```env
# .env æ•ˆèƒ½è¨­å®š
FP16=true                    # åŠç²¾åº¦æµ®é»æ•¸
MAX_CONCURRENCY=2            # é™åˆ¶ä¸¦ç™¼æ•¸
TORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### CPU æ¨¡å¼è¨­å®š
```env
# CPU æ¨¡å¼ (ç„¡ GPU æ™‚)
DEVICE=cpu
FP16=false
MAX_CONCURRENCY=1
```

### ç¶²è·¯å„ªåŒ–
```env
# ç¶²è·¯å¿«å–è¨­å®š
HF_HUB_CACHE=${AI_CACHE_ROOT}/hf/hub
HF_DATASETS_OFFLINE=1        # é›¢ç·šæ¨¡å¼
TRANSFORMERS_OFFLINE=1
```

## ğŸ” é©—è­‰å®‰è£

### å®Œæ•´æ¸¬è©¦è…³æœ¬
```bash
#!/bin/bash
# test_installation.sh

echo "ğŸ§ª Voice App Installation Test"
echo "=============================="

# æ¸¬è©¦ Python ç’°å¢ƒ
python --version
pip list | grep -E "(torch|transformers|fastapi)"

# æ¸¬è©¦ Node.js
node --version
npm --version

# æ¸¬è©¦ FFmpeg
ffmpeg -version | head -1

# æ¸¬è©¦ API
python scripts/test_api.py --quick

# æ¸¬è©¦å‰ç«¯
cd frontend/react_app && npm run build
cd ../gradio_app && python -c "import gradio; print('Gradio OK')"
cd ../pyqt_app && python -c "from PyQt6.QtWidgets import QApplication; print('PyQt6 OK')"

echo "âœ… Installation test completed!"
```

### æ•ˆèƒ½åŸºæº–æ¸¬è©¦
```bash
# åŸ·è¡ŒåŸºæº–æ¸¬è©¦
python scripts/benchmark.py

# é æœŸçµæœ:
# TTS Processing: ~3-8s per sentence
# VC Processing: ~5-15s per audio file
# GPU Memory Usage: <4GB for standard models
```

---

# docs/API.md - API å®Œæ•´æ–‡æª”

## ğŸ”Œ Voice App API v1 Reference

Base URL: `http://localhost:8000/api/v1`

## èªè­‰èˆ‡æˆæ¬Š

ç›®å‰ç‰ˆæœ¬ç‚ºæœ¬åœ°é–‹ç™¼ä½¿ç”¨ï¼Œç„¡éœ€èªè­‰ã€‚æœªä¾†ç‰ˆæœ¬å°‡æ”¯æ´ API Key èªè­‰ã€‚

## éŒ¯èª¤è™•ç†

æ‰€æœ‰ API ç«¯é»éµå¾ªçµ±ä¸€éŒ¯èª¤æ ¼å¼ï¼š

```json
{
  "error": "Error message description",
  "code": "ERROR_CODE",
  "details": {
    "field": "Additional error details"
  }
}
```

HTTP ç‹€æ…‹ç¢¼ï¼š
- `200`: æˆåŠŸ
- `400`: è«‹æ±‚åƒæ•¸éŒ¯èª¤
- `422`: é©—è­‰å¤±æ•—
- `500`: ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤
- `503`: æœå‹™æš«æ™‚ä¸å¯ç”¨

## ç«¯é»åˆ—è¡¨

### å¥åº·æª¢æŸ¥

#### GET /healthz

æª¢æŸ¥å¾Œç«¯æœå‹™ç‹€æ…‹ã€‚

**å›æ‡‰:**
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "uptime": 3600.5,
  "gpu_available": true,
  "models_loaded": {
    "tts": "xtts",
    "vc": "rvc"
  }
}
```

### æ–‡å­—è½‰èªéŸ³ (TTS)

#### POST /api/v1/tts

å°‡æ–‡å­—è½‰æ›ç‚ºèªéŸ³ã€‚

**è«‹æ±‚é«”:**
```json
{
  "text": "è¦åˆæˆçš„æ–‡å­—å…§å®¹",
  "speaker_id": "default",
  "language": "zh",
  "speed": 1.0,
  "emotion": "neutral"
}
```

**åƒæ•¸èªªæ˜:**
- `text` (å¿…éœ€): è¦åˆæˆçš„æ–‡å­—ï¼Œæœ€å¤§ 1000 å­—å…ƒ
- `speaker_id` (å¯é¸): èªªè©±è€… IDï¼Œé è¨­ "default"
- `language` (å¯é¸): èªè¨€ä»£ç¢¼ ("zh", "en", "ja")ï¼Œé è¨­ "zh"
- `speed` (å¯é¸): èªé€Ÿå€ç‡ (0.5-2.0)ï¼Œé è¨­ 1.0
- `emotion` (å¯é¸): æƒ…æ„Ÿ ("neutral", "happy", "sad")ï¼Œé è¨­ "neutral"

**å›æ‡‰:**
```json
{
  "audio_url": "/outputs/tts_20241201_123456.wav",
  "duration": 5.2,
  "processing_time": 3.1,
  "sample_rate": 22050,
  "file_size": 230400
}
```

**ç¯„ä¾‹:**
```bash
curl -X POST http://localhost:8000/api/v1/tts \
  -H "Content-Type: application/json" \
  -d '{
    "text": "æ­¡è¿ä½¿ç”¨èªéŸ³åˆæˆç³»çµ±",
    "speaker_id": "female_zh",
    "language": "zh",
    "speed": 1.2
  }'
```

### èªéŸ³è½‰æ› (VC)

#### POST /api/v1/vc

å°‡èªéŸ³è½‰æ›ç‚ºç›®æ¨™èªªè©±è€…è²ç·šã€‚

**è«‹æ±‚é«”:**
```json
{
  "source_audio": "base64_encoded_audio_data",
  "target_speaker": "speaker_001",
  "preserve_pitch": true,
  "noise_reduction": true
}
```

**åƒæ•¸èªªæ˜:**
- `source_audio` (å¿…éœ€): Base64 ç·¨ç¢¼çš„éŸ³è¨Šè³‡æ–™
- `target_speaker` (å¿…éœ€): ç›®æ¨™èªªè©±è€… ID
- `preserve_pitch` (å¯é¸): æ˜¯å¦ä¿æŒéŸ³èª¿ï¼Œé è¨­ true
- `noise_reduction` (å¯é¸): æ˜¯å¦é™å™ªï¼Œé è¨­ false

**å›æ‡‰:**
```json
{
  "audio_url": "/outputs/vc_20241201_123456.wav",
  "processing_time": 8.5,
  "sample_rate": 22050,
  "file_size": 451200
}
```

**ç¯„ä¾‹:**
```bash
# ä¸Šå‚³éŸ³æª”ä¸¦è½‰æ›
curl -X POST http://localhost:8000/api/v1/vc \
  -H "Content-Type: application/json" \
  -d '{
    "source_audio": "'$(base64 -w 0 input.wav)'",
    "target_speaker": "male_en",
    "preserve_pitch": true
  }'
```

### èªªè©±è€…è³‡æ–™

#### GET /api/v1/profiles

å–å¾—æ‰€æœ‰å¯ç”¨çš„èªªè©±è€…è³‡æ–™ã€‚

**å›æ‡‰:**
```json
{
  "profiles": [
    {
      "id": "default",
      "name": "é è¨­èªªè©±è€…",
      "language": "zh",
      "gender": "neutral",
      "description": "ç³»çµ±é è¨­èªªè©±è€…",
      "sample_audio": "/samples/default.wav",
      "created_at": "2024-01-01T00:00:00Z"
    }
  ],
  "total": 1
}
```

#### POST /api/v1/profiles

å»ºç«‹æ–°çš„èªªè©±è€…è³‡æ–™ã€‚

**è«‹æ±‚é«”:**
```json
{
  "name": "æ–°èªªè©±è€…",
  "language": "zh",
  "gender": "female",
  "description": "è‡ªè¨‚å¥³æ€§èªªè©±è€…",
  "sample_audio": "base64_encoded_sample"
}
```

#### GET /api/v1/profiles/{profile_id}

å–å¾—ç‰¹å®šèªªè©±è€…è³‡æ–™ã€‚

#### PUT /api/v1/profiles/{profile_id}

æ›´æ–°èªªè©±è€…è³‡æ–™ã€‚

#### DELETE /api/v1/profiles/{profile_id}

åˆªé™¤èªªè©±è€…è³‡æ–™ã€‚

### æ‰¹æ¬¡è™•ç†

#### POST /api/v1/batch/tts

æ‰¹æ¬¡æ–‡å­—è½‰èªéŸ³ã€‚

**è«‹æ±‚é«”:**
```json
{
  "texts": [
    "ç¬¬ä¸€æ®µæ–‡å­—",
    "ç¬¬äºŒæ®µæ–‡å­—",
    "ç¬¬ä¸‰æ®µæ–‡å­—"
  ],
  "speaker_id": "default",
  "language": "zh",
  "speed": 1.0
}
```

**å›æ‡‰:**
```json
{
  "job_id": "batch_20241201_123456",
  "status": "processing",
  "results": [
    {
      "index": 0,
      "audio_url": "/outputs/batch_0_20241201_123456.wav",
      "duration": 3.2
    }
  ],
  "progress": {
    "completed": 1,
    "total": 3,
    "percentage": 33.3
  }
}
```

#### GET /api/v1/batch/{job_id}

æŸ¥è©¢æ‰¹æ¬¡å·¥ä½œç‹€æ…‹ã€‚

### éŸ³è¨Šè™•ç†

#### POST /api/v1/audio/normalize

éŸ³è¨Šæ­£è¦åŒ–è™•ç†ã€‚

**è«‹æ±‚é«”:**
```json
{
  "audio": "base64_encoded_audio",
  "target_lufs": -16.0,
  "remove_silence": true
}
```

#### POST /api/v1/audio/convert

éŸ³è¨Šæ ¼å¼è½‰æ›ã€‚

**è«‹æ±‚é«”:**
```json
{
  "audio": "base64_encoded_audio",
  "output_format": "wav",
  "sample_rate": 22050,
  "bit_depth": 16
}
```

## ğŸ”§ SDK èˆ‡å®¢æˆ¶ç«¯

### Python SDK

```python
from voice_app_client import VoiceAPIClient

# åˆå§‹åŒ–å®¢æˆ¶ç«¯
client = VoiceAPIClient("http://localhost:8000")

# TTS åˆæˆ
result = client.text_to_speech(
    text="Hello World",
    speaker_id="default",
    language="en"
)

# VC è½‰æ›
with open("input.wav", "rb") as f:
    result = client.voice_conversion(
        audio_file=f.read(),
        target_speaker="speaker_001"
    )
```

### JavaScript SDK

```javascript
import { VoiceAPIClient } from './voice-app-client.js';

const client = new VoiceAPIClient('http://localhost:8000');

// TTS åˆæˆ
const ttsResult = await client.textToSpeech('Hello World', {
  speakerId: 'default',
  language: 'en'
});

// VC è½‰æ›
const vcResult = await client.voiceConversion(audioFile, 'speaker_001');
```

## ğŸ“Š é™åˆ¶èˆ‡é…é¡

| é …ç›® | é™åˆ¶ |
|------|------|
| æ–‡å­—é•·åº¦ | 1000 å­—å…ƒ |
| éŸ³è¨Šæª”æ¡ˆå¤§å° | 50MB |
| ä¸¦ç™¼è«‹æ±‚æ•¸ | å¯è¨­å®š (é è¨­ 2) |
| æ‰¹æ¬¡è™•ç†æ•¸é‡ | 100 é …ç›® |
| API å‘¼å«é »ç‡ | ç„¡é™åˆ¶ (æœ¬åœ°éƒ¨ç½²) |

## ğŸ”’ å®‰å…¨æ€§

### è¼¸å…¥é©—è­‰
- æ‰€æœ‰è¼¸å…¥åƒæ•¸éƒ½ç¶“éé©—è­‰å’Œæ¸…ç†
- æª”æ¡ˆé¡å‹æª¢æŸ¥èˆ‡å¤§å°é™åˆ¶
- SQL æ³¨å…¥é˜²è­·

### è³‡æ–™éš±ç§
- æ‰€æœ‰éŸ³è¨Šè™•ç†åœ¨æœ¬åœ°é€²è¡Œ
- ä¸æœƒä¸Šå‚³è³‡æ–™åˆ°å¤–éƒ¨æœå‹™
- å¯è¨­å®šè‡ªå‹•æ¸…ç†æš«å­˜æª”æ¡ˆ

---

# docs/DEVELOPMENT.md - é–‹ç™¼æŒ‡å—

## ğŸ‘¨â€ğŸ’» Voice App é–‹ç™¼æŒ‡å—

æœ¬æ–‡æª”èªªæ˜ Voice App çš„é–‹ç™¼å·¥ä½œæµç¨‹ã€ç¨‹å¼ç¢¼è¦ç¯„å’Œæœ€ä½³å¯¦è¸ã€‚

## ğŸ—ï¸ å°ˆæ¡ˆçµæ§‹è©³è§£

### å¾Œç«¯æ¶æ§‹ (backend/)

```
backend/
â”œâ”€â”€ api/                        # FastAPI æ‡‰ç”¨å±¤
â”‚   â”œâ”€â”€ dependencies.py        # ä¾è³´æ³¨å…¥èˆ‡ä¸­é–“ä»¶
â”‚   â”œâ”€â”€ middleware.py          # CORSã€æ—¥èªŒã€éŒ¯èª¤è™•ç†
â”‚   â”œâ”€â”€ main.py                # FastAPI æ‡‰ç”¨å…¥å£
â”‚   â””â”€â”€ routers/               # API è·¯ç”±æ¨¡çµ„
â”‚       â”œâ”€â”€ health.py          # å¥åº·æª¢æŸ¥ç«¯é»
â”‚       â”œâ”€â”€ tts.py             # TTS API è·¯ç”±
â”‚       â”œâ”€â”€ vc.py              # VC API è·¯ç”±
â”‚       â”œâ”€â”€ batch.py           # æ‰¹æ¬¡è™•ç†è·¯ç”±
â”‚       â””â”€â”€ profiles.py        # èªªè©±è€…ç®¡ç†è·¯ç”±
â”œâ”€â”€ core/                      # æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
â”‚   â”œâ”€â”€ config.py              # Pydantic è¨­å®šç®¡ç†
â”‚   â”œâ”€â”€ shared_cache.py        # AI å¿«å–ç³»çµ±
â”‚   â”œâ”€â”€ model_manager.py       # æ¨¡å‹è¼‰å…¥èˆ‡ç®¡ç†
â”‚   â”œâ”€â”€ performance.py         # æ•ˆèƒ½å„ªåŒ–è¨­å®š
â”‚   â””â”€â”€ audio/                 # éŸ³è¨Šè™•ç†å·¥å…·
â”‚       â”œâ”€â”€ io.py              # éŸ³è¨Š I/O èˆ‡è½‰æ›
â”‚       â””â”€â”€ normalization.py   # éŸ³é‡æ­£è¦åŒ–
â”œâ”€â”€ services/                  # æ¥­å‹™é‚è¼¯æœå‹™
â”‚   â”œâ”€â”€ tts_service.py         # TTS å¼•æ“å°è£
â”‚   â””â”€â”€ vc_service.py          # VC å¼•æ“å°è£
â””â”€â”€ tests/                     # æ¸¬è©¦å¥—ä»¶
    â”œâ”€â”€ test_health.py         # å¥åº·æª¢æŸ¥æ¸¬è©¦
    â”œâ”€â”€ test_tts.py            # TTS API æ¸¬è©¦
    â””â”€â”€ test_vc.py             # VC API æ¸¬è©¦
```

### å‰ç«¯æ¶æ§‹ (frontend/)

```
frontend/
â”œâ”€â”€ shared/                    # è·¨å¹³å°å…±ç”¨æ¨¡çµ„
â”‚   â”œâ”€â”€ api_client.py          # Python API å®¢æˆ¶ç«¯
â”‚   â””â”€â”€ api_client.js          # JavaScript API å®¢æˆ¶ç«¯
â”œâ”€â”€ react_app/                 # React Web æ‡‰ç”¨
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # å¯è¤‡ç”¨çµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ pages/             # é é¢çµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ services/          # API æœå‹™å±¤
â”‚   â”‚   â”œâ”€â”€ styles/            # CSS æ¨£å¼æª”æ¡ˆ
â”‚   â”‚   â””â”€â”€ utils/             # å·¥å…·å‡½æ•¸
â”‚   â”œâ”€â”€ package.json           # NPM ä¾è³´ç®¡ç†
â”‚   â””â”€â”€ webpack.config.js      # Webpack é…ç½®
â”œâ”€â”€ gradio_app/                # Gradio ç ”ç©¶ç•Œé¢
â”‚   â”œâ”€â”€ components/            # Gradio ç•Œé¢çµ„ä»¶
â”‚   â”œâ”€â”€ styles/                # è‡ªè¨‚æ¨£å¼èˆ‡ä¸»é¡Œ
â”‚   â””â”€â”€ app.py                 # Gradio æ‡‰ç”¨å…¥å£
â””â”€â”€ pyqt_app/                  # PyQt æ¡Œé¢æ‡‰ç”¨
    â”œâ”€â”€ widgets/               # PyQt ç•Œé¢å…ƒä»¶
    â”œâ”€â”€ styles/                # QSS æ¨£å¼æª”æ¡ˆ
    â”œâ”€â”€ utils/                 # æ¡Œé¢æ‡‰ç”¨å·¥å…·
    â””â”€â”€ main.py                # PyQt æ‡‰ç”¨å…¥å£
```

## ğŸ”„ é–‹ç™¼å·¥ä½œæµç¨‹

### 1. é–‹ç™¼ç’°å¢ƒè¨­å®š

```bash
# å…‹éš†å°ˆæ¡ˆä¸¦åˆ‡æ›åˆ°é–‹ç™¼åˆ†æ”¯
git clone https://github.com/your-username/voice-app.git
cd voice-app
git checkout develop

# å»ºç«‹é–‹ç™¼ç’°å¢ƒ
conda create -n voice-app-dev python=3.10
conda activate voice-app-dev

# å®‰è£é–‹ç™¼ä¾è³´
python scripts/setup_dev.py

# å®‰è£é–‹ç™¼å·¥å…·
pip install black isort flake8 pytest pre-commit
```

### 2. Git åˆ†æ”¯ç­–ç•¥

æˆ‘å€‘ä½¿ç”¨ Git Flow åˆ†æ”¯æ¨¡å‹ï¼š

- `main`: ç©©å®šç™¼å¸ƒç‰ˆæœ¬
- `develop`: é–‹ç™¼æ•´åˆåˆ†æ”¯
- `feature/*`: åŠŸèƒ½é–‹ç™¼åˆ†æ”¯
- `hotfix/*`: ç·Šæ€¥ä¿®å¾©åˆ†æ”¯
- `release/*`: ç™¼å¸ƒæº–å‚™åˆ†æ”¯

```bash
# å»ºç«‹åŠŸèƒ½åˆ†æ”¯
git checkout develop
git pull origin develop
git checkout -b feature/add-bark-tts-engine

# é–‹ç™¼å®Œæˆå¾Œæäº¤
git add .
git commit -m "feat(tts): add Bark engine support with emotional control"

# æ¨é€ä¸¦å»ºç«‹ PR
git push origin feature/add-bark-tts-engine
```

### 3. ç¨‹å¼ç¢¼è¦ç¯„

#### Python ç¨‹å¼ç¢¼é¢¨æ ¼ (PEP 8)

```python
# æª”æ¡ˆé ‚éƒ¨è¨»é‡‹
"""
TTS Service Module

Provides text-to-speech functionality with support for multiple engines
including XTTS, OpenVoice, and Bark.
"""

import os
from typing import Dict, List, Optional, Union
from pathlib import Path

# é¡åˆ¥å®šç¾©
class TTSService:
    """Text-to-Speech service with multi-engine support."""

    def __init__(self, engine: str = "xtts", device: str = "cuda"):
        """
        Initialize TTS service.

        Args:
            engine: TTS engine name ("xtts", "openvoice", "bark")
            device: Computation device ("cuda", "cpu")
        """
        self.engine = engine
        self.device = device
        self._model = None

    async def synthesize(
        self,
        text: str,
        speaker_id: str = "default",
        **kwargs
    ) -> Dict[str, Union[str, float]]:
        """
        Synthesize speech from text.

        Args:
            text: Input text to synthesize
            speaker_id: Speaker identifier
            **kwargs: Additional engine-specific parameters

        Returns:
            Dictionary containing audio_url and metadata

        Raises:
            ValueError: If text is empty or too long
            RuntimeError: If synthesis fails
        """
        if not text.strip():
            raise ValueError("Text cannot be empty")

        # Implementation here...
        return {
            "audio_url": "/outputs/tts_example.wav",
            "duration": 5.2,
            "processing_time": 3.1
        }
```

#### JavaScript ç¨‹å¼ç¢¼é¢¨æ ¼ (ESLint + Prettier)

```javascript
/**
 * Audio Player Component
 *
 * Provides audio playback controls with progress tracking
 */
import React, { useState, useRef, useEffect } from 'react';

const AudioPlayer = ({ src, filename = 'audio.wav', onDownload }) => {
  const audioRef = useRef(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);

  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;

    const updateTime = () => setCurrentTime(audio.currentTime);
    const updateDuration = () => setDuration(audio.duration || 0);

    audio.addEventListener('timeupdate', updateTime);
    audio.addEventListener('loadedmetadata', updateDuration);

    return () => {
      audio.removeEventListener('timeupdate', updateTime);
      audio.removeEventListener('loadedmetadata', updateDuration);
    };
  }, [src]);

  // Component implementation...
};

export default AudioPlayer;
```

### 4. æ¸¬è©¦ç­–ç•¥

#### å–®å…ƒæ¸¬è©¦ (pytest)

```python
# tests/test_tts_service.py
import pytest
from unittest.mock import Mock, patch
from backend.services.tts_service import TTSService

class TestTTSService:
    """Test suite for TTS service."""

    @pytest.fixture
    def tts_service(self):
        """Create TTS service instance for testing."""
        return TTSService(engine="xtts", device="cpu")

    @pytest.mark.asyncio
    async def test_synthesize_success(self, tts_service):
        """Test successful text synthesis."""
        result = await tts_service.synthesize("Hello World")

        assert "audio_url" in result
        assert result["duration"] > 0
        assert result["processing_time"] > 0

    @pytest.mark.asyncio
    async def test_synthesize_empty_text(self, tts_service):
        """Test synthesis with empty text raises ValueError."""
        with pytest.raises(ValueError, match="Text cannot be empty"):
            await tts_service.synthesize("")

    @patch('backend.services.tts_service.torch')
    def test_model_loading(self, mock_torch, tts_service):
        """Test model loading with mocked torch."""
        mock_torch.cuda.is_available.return_value = True
        # Test implementation...
```

#### æ•´åˆæ¸¬è©¦

```python
# tests/test_api_integration.py
import pytest
from fastapi.testclient import TestClient
from backend.api.main import app

client = TestClient(app)

def test_health_endpoint():
    """Test health check endpoint."""
    response = client.get("/healthz")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "healthy"

def test_tts_endpoint():
    """Test TTS endpoint with valid input."""
    payload = {
        "text": "Test synthesis",
        "speaker_id": "default",
        "language": "en"
    }
    response = client.post("/api/v1/tts", json=payload)
    assert response.status_code == 200
    data = response.json()
    assert "audio_url" in data
```

#### å‰ç«¯æ¸¬è©¦ (Jest + React Testing Library)

```javascript
// frontend/react_app/src/components/__tests__/AudioPlayer.test.js
import React from 'react';
import { render, screen, fireEvent } from '@testing-library/react';
import AudioPlayer from '../AudioPlayer';

describe('AudioPlayer', () => {
  const mockProps = {
    src: '/test/audio.wav',
    filename: 'test-audio.wav',
    onDownload: jest.fn()
  };

  test('renders audio player with controls', () => {
    render(<AudioPlayer {...mockProps} />);

    expect(screen.getByRole('button', { name: /play/i })).toBeInTheDocument();
    expect(screen.getByRole('button', { name: /stop/i })).toBeInTheDocument();
  });

  test('calls onDownload when download button clicked', () => {
    render(<AudioPlayer {...mockProps} />);

    const downloadBtn = screen.getByRole('button', { name: /download/i });
    fireEvent.click(downloadBtn);

    expect(mockProps.onDownload).toHaveBeenCalledWith(
      mockProps.src,
      mockProps.filename
    );
  });
});
```

### 5. ç¨‹å¼ç¢¼å“è³ªå·¥å…·

#### è‡ªå‹•æ ¼å¼åŒ–èˆ‡æª¢æŸ¥

```bash
# Python ç¨‹å¼ç¢¼æ ¼å¼åŒ–
black backend/ scripts/
isort backend/ scripts/

# ç¨‹å¼ç¢¼é¢¨æ ¼æª¢æŸ¥
flake8 backend/ scripts/
mypy backend/ scripts/

# JavaScript æ ¼å¼åŒ–
cd frontend/react_app
npm run lint
npm run format
```

#### Pre-commit æ›é‰¤

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3.10

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8

  - repo: https://github.com/pre-commit/mirrors-eslint
    rev: v8.44.0
    hooks:
      - id: eslint
        files: \.(js|jsx)$
        additional_dependencies:
          - eslint@8.44.0
          - eslint-plugin-react@7.32.2
```

## ğŸ›ï¸ æ¶æ§‹è¨­è¨ˆåŸå‰‡

### 1. æ¨¡çµ„åŒ–è¨­è¨ˆ

æ¯å€‹åŠŸèƒ½æ¨¡çµ„éƒ½æ‡‰è©²ï¼š
- **å–®ä¸€è·è²¬**: æ¯å€‹æ¨¡çµ„åªè™•ç†ä¸€ç¨®åŠŸèƒ½
- **é¬†æ•£è€¦åˆ**: æ¨¡çµ„é–“ä¾è³´æœ€å°åŒ–
- **é«˜å…§èš**: ç›¸é—œåŠŸèƒ½èšé›†åœ¨åŒä¸€æ¨¡çµ„

```python
# è‰¯å¥½çš„æ¨¡çµ„åŒ–è¨­è¨ˆç¯„ä¾‹
class ModelManager:
    """çµ±ä¸€çš„æ¨¡å‹ç®¡ç†ä»‹é¢"""

    def load_model(self, model_type: str, engine: str) -> None:
        """è¼‰å…¥æŒ‡å®šé¡å‹å’Œå¼•æ“çš„æ¨¡å‹"""
        pass

    def unload_model(self, model_type: str) -> None:
        """å¸è¼‰æŒ‡å®šé¡å‹çš„æ¨¡å‹"""
        pass

    def switch_engine(self, model_type: str, new_engine: str) -> None:
        """åˆ‡æ›æ¨¡å‹å¼•æ“"""
        pass
```

### 2. ä¾è³´æ³¨å…¥

ä½¿ç”¨ä¾è³´æ³¨å…¥æ¨¡å¼é™ä½è€¦åˆåº¦ï¼š

```python
# backend/api/dependencies.py
from fastapi import Depends
from backend.core.config import Settings
from backend.services.tts_service import TTSService

def get_settings() -> Settings:
    return Settings()

def get_tts_service(settings: Settings = Depends(get_settings)) -> TTSService:
    return TTSService(
        engine=settings.TTS_ENGINE,
        device=settings.DEVICE
    )

# åœ¨è·¯ç”±ä¸­ä½¿ç”¨
@router.post("/tts")
async def synthesize_speech(
    request: TTSRequest,
    tts_service: TTSService = Depends(get_tts_service)
):
    return await tts_service.synthesize(request.text)
```

### 3. éŒ¯èª¤è™•ç†ç­–ç•¥

çµ±ä¸€çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„ï¼š

```python
# backend/core/exceptions.py
class VoiceAppException(Exception):
    """åŸºç¤ä¾‹å¤–é¡"""
    def __init__(self, message: str, code: str = "VOICE_APP_ERROR"):
        self.message = message
        self.code = code
        super().__init__(self.message)

class TTSError(VoiceAppException):
    """TTS ç›¸é—œéŒ¯èª¤"""
    pass

class VCError(VoiceAppException):
    """VC ç›¸é—œéŒ¯èª¤"""
    pass

# å…¨åŸŸéŒ¯èª¤è™•ç†å™¨
@app.exception_handler(VoiceAppException)
async def voice_app_exception_handler(request: Request, exc: VoiceAppException):
    logger.error(f"Voice App Error: {exc.code} - {exc.message}")
    return JSONResponse(
        status_code=400,
        content={
            "error": exc.message,
            "code": exc.code
        }
    )
```

## ğŸ”§ é–‹ç™¼æœ€ä½³å¯¦è¸

### 1. ç•°æ­¥ç¨‹å¼è¨­è¨ˆ

ä½¿ç”¨ async/await æå‡æ•ˆèƒ½ï¼š

```python
import asyncio
from typing import List

class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨"""

    async def process_batch_tts(
        self,
        texts: List[str],
        concurrency: int = 3
    ) -> List[Dict]:
        """ä¸¦è¡Œè™•ç†å¤šå€‹ TTS è«‹æ±‚"""
        semaphore = asyncio.Semaphore(concurrency)

        async def process_single(text: str) -> Dict:
            async with semaphore:
                return await self.tts_service.synthesize(text)

        tasks = [process_single(text) for text in texts]
        return await asyncio.gather(*tasks)
```

### 2. å¿«å–ç­–ç•¥

å¯¦ç¾å¤šå±¤å¿«å–æå‡æ•ˆèƒ½ï¼š

```python
# backend/core/cache.py
import asyncio
from typing import Optional, Any
from functools import wraps

class CacheManager:
    """çµ±ä¸€å¿«å–ç®¡ç†"""

    def __init__(self):
        self._memory_cache = {}
        self._disk_cache_path = Path("cache/")

    async def get(self, key: str) -> Optional[Any]:
        """å¾å¿«å–ç²å–è³‡æ–™"""
        # å…ˆæª¢æŸ¥è¨˜æ†¶é«”å¿«å–
        if key in self._memory_cache:
            return self._memory_cache[key]

        # å†æª¢æŸ¥ç£ç¢Ÿå¿«å–
        disk_path = self._disk_cache_path / f"{key}.pkl"
        if disk_path.exists():
            import pickle
            with open(disk_path, 'rb') as f:
                data = pickle.load(f)
                self._memory_cache[key] = data  # è¼‰å…¥åˆ°è¨˜æ†¶é«”
                return data

        return None

    async def set(self, key: str, value: Any, disk: bool = True):
        """è¨­å®šå¿«å–è³‡æ–™"""
        self._memory_cache[key] = value

        if disk:
            import pickle
            self._disk_cache_path.mkdir(exist_ok=True)
            with open(self._disk_cache_path / f"{key}.pkl", 'wb') as f:
                pickle.dump(value, f)

def cache_result(cache_key_func):
    """å¿«å–è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = cache_key_func(*args, **kwargs)

            # å˜—è©¦å¾å¿«å–ç²å–
            cached = await cache_manager.get(cache_key)
            if cached is not None:
                return cached

            # åŸ·è¡Œå‡½æ•¸ä¸¦å¿«å–çµæœ
            result = await func(*args, **kwargs)
            await cache_manager.set(cache_key, result)
            return result
        return wrapper
    return decorator
```

### 3. ç›£æ§èˆ‡æ—¥èªŒ

å®Œå–„çš„ç›£æ§å’Œæ—¥èªŒç³»çµ±ï¼š

```python
# backend/core/monitoring.py
import logging
import time
from functools import wraps
from typing import Dict, Any

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/voice_app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def monitor_performance(func):
    """æ•ˆèƒ½ç›£æ§è£é£¾å™¨"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time

            logger.info(f"{func.__name__} completed in {duration:.2f}s")
            return result

        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"{func.__name__} failed after {duration:.2f}s: {e}")
            raise

    return wrapper

class MetricsCollector:
    """æ•ˆèƒ½æŒ‡æ¨™æ”¶é›†å™¨"""

    def __init__(self):
        self.metrics = {
            "tts_requests": 0,
            "vc_requests": 0,
            "total_processing_time": 0.0,
            "error_count": 0
        }

    def record_tts_request(self, processing_time: float):
        """è¨˜éŒ„ TTS è«‹æ±‚"""
        self.metrics["tts_requests"] += 1
        self.metrics["total_processing_time"] += processing_time

    def record_error(self):
        """è¨˜éŒ„éŒ¯èª¤"""
        self.metrics["error_count"] += 1

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡æ–™"""
        total_requests = self.metrics["tts_requests"] + self.metrics["vc_requests"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / total_requests
            if total_requests > 0 else 0
        )

        return {
            **self.metrics,
            "total_requests": total_requests,
            "average_processing_time": avg_processing_time,
            "error_rate": self.metrics["error_count"] / total_requests if total_requests > 0 else 0
        }
```

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–æŒ‡å—

### 1. GPU è¨˜æ†¶é«”ç®¡ç†

```python
import torch
from contextlib import contextmanager

@contextmanager
def gpu_memory_guard():
    """GPU è¨˜æ†¶é«”ä¿è­·ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

class ModelLoader:
    """æ™ºèƒ½æ¨¡å‹è¼‰å…¥å™¨"""

    def __init__(self, max_models: int = 2):
        self.loaded_models = {}
        self.max_models = max_models
        self.access_count = {}

    def load_model(self, model_key: str, loader_func):
        """è¼‰å…¥æ¨¡å‹ï¼Œæ”¯æ´ LRU å¿«å–"""
        if model_key in self.loaded_models:
            self.access_count[model_key] += 1
            return self.loaded_models[model_key]

        # å¦‚æœè¶…éæœ€å¤§æ¨¡å‹æ•¸ï¼Œå¸è¼‰æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹
        if len(self.loaded_models) >= self.max_models:
            lru_key = min(self.access_count, key=self.access_count.get)
            self.unload_model(lru_key)

        with gpu_memory_guard():
            model = loader_func()
            self.loaded_models[model_key] = model
            self.access_count[model_key] = 1

        return model

    def unload_model(self, model_key: str):
        """å¸è¼‰æ¨¡å‹é‡‹æ”¾è¨˜æ†¶é«”"""
        if model_key in self.loaded_models:
            del self.loaded_models[model_key]
            del self.access_count[model_key]

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
```

### 2. éŸ³è¨Šè™•ç†å„ªåŒ–

```python
import librosa
import numpy as np
from concurrent.futures import ThreadPoolExecutor

class AudioProcessor:
    """é«˜æ•ˆéŸ³è¨Šè™•ç†å™¨"""

    def __init__(self, target_sr: int = 22050):
        self.target_sr = target_sr
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def process_audio_async(self, audio_path: str) -> np.ndarray:
        """ç•°æ­¥éŸ³è¨Šè™•ç†"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._process_audio_sync,
            audio_path
        )

    def _process_audio_sync(self, audio_path: str) -> np.ndarray:
        """åŒæ­¥éŸ³è¨Šè™•ç†æ ¸å¿ƒé‚è¼¯"""
        # è¼‰å…¥éŸ³è¨Š
        audio, sr = librosa.load(audio_path, sr=None)

        # é‡æ–°å–æ¨£
        if sr != self.target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=self.target_sr)

        # æ­£è¦åŒ–
        audio = self._normalize_audio(audio)

        return audio

    def _normalize_audio(self, audio: np.ndarray, target_lufs: float = -16.0) -> np.ndarray:
        """éŸ³é‡æ­£è¦åŒ–åˆ°ç›®æ¨™ LUFS"""
        try:
            import pyloudnorm as pyln
            meter = pyln.Meter(self.target_sr)
            loudness = meter.integrated_loudness(audio)
            normalized_audio = pyln.normalize.loudness(audio, loudness, target_lufs)
            return normalized_audio
        except ImportError:
            # å›é€€åˆ°ç°¡å–®æ­£è¦åŒ–
            return audio / np.max(np.abs(audio)) * 0.95
```

## ğŸš€ éƒ¨ç½²èˆ‡ç™¼å¸ƒ

### 1. ç’°å¢ƒåˆ†é›¢

ä½¿ç”¨ä¸åŒçš„é…ç½®æª”æ¡ˆç®¡ç†å„ç’°å¢ƒï¼š

```python
# backend/core/config.py
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """æ‡‰ç”¨è¨­å®š"""

    # ç’°å¢ƒæ¨™è­˜
    ENV: str = "development"
    DEBUG: bool = True

    # ä¼ºæœå™¨è¨­å®š
    HOST: str = "0.0.0.0"
    PORT: int = 8000

    # AI è¨­å®š
    DEVICE: str = "cuda"
    FP16: bool = True
    TTS_ENGINE: str = "xtts"
    VC_ENGINE: str = "rvc"

    # è·¯å¾‘è¨­å®š
    AI_CACHE_ROOT: str = "/tmp/voice-app-cache"
    OUTPUT_DIR: Optional[str] = None
    SPEAKER_DIR: Optional[str] = None

    # æ•ˆèƒ½è¨­å®š
    MAX_CONCURRENCY: int = 2
    MODEL_CACHE_SIZE: int = 3

    class Config:
        env_file = ".env"

    def __post_init__(self):
        """è¨­å®šå¾Œè™•ç†"""
        if self.OUTPUT_DIR is None:
            self.OUTPUT_DIR = f"{self.AI_CACHE_ROOT}/outputs"
        if self.SPEAKER_DIR is None:
            self.SPEAKER_DIR = f"{self.AI_CACHE_ROOT}/speakers"

# ç’°å¢ƒç‰¹å®šè¨­å®š
class DevelopmentSettings(Settings):
    DEBUG: bool = True
    LOG_LEVEL: str = "DEBUG"

class ProductionSettings(Settings):
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    MAX_CONCURRENCY: int = 4

def get_settings() -> Settings:
    """æ ¹æ“šç’°å¢ƒè®Šæ•¸è¿”å›å°æ‡‰è¨­å®š"""
    env = os.getenv("ENV", "development")

    if env == "production":
        return ProductionSettings()
    else:
        return DevelopmentSettings()
```

### 2. CI/CD å·¥ä½œæµç¨‹

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ develop, main ]
  pull_request:
    branches: [ develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install -r backend/requirements-dev.txt

    - name: Run tests
      run: |
        cd backend
        python -m pytest tests/ -v --cov=. --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml

  lint:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10

    - name: Install linting tools
      run: |
        pip install black isort flake8 mypy

    - name: Run linting
      run: |
        black --check backend/ scripts/
        isort --check-only backend/ scripts/
        flake8 backend/ scripts/
        mypy backend/

  frontend-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    - name: Install frontend dependencies
      run: |
        cd frontend/react_app
        npm ci

    - name: Run frontend tests
      run: |
        cd frontend/react_app
        npm run test -- --coverage --watchAll=false

    - name: Build frontend
      run: |
        cd frontend/react_app
        npm run build
```

### 3. Docker åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile.backend
FROM python:3.10-slim

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹æ‡‰ç”¨ç›®éŒ„
WORKDIR /app

# è¤‡è£½ä¸¦å®‰è£ Python ä¾è³´
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼ç¢¼
COPY backend/ ./backend/
COPY frontend/shared/ ./frontend/shared/

# å»ºç«‹å¿«å–ç›®éŒ„
RUN mkdir -p /app/cache/models /app/cache/outputs

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV AI_CACHE_ROOT=/app/cache
ENV PYTHONPATH=/app

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/healthz || exit 1

# å•Ÿå‹•æŒ‡ä»¤
CMD ["python", "-m", "backend.api.main"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  voice-app-backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - DEVICE=cpu
      - AI_CACHE_ROOT=/app/cache
    volumes:
      - ./cache:/app/cache
      - ./logs:/app/logs
    restart: unless-stopped

  voice-app-frontend:
    build:
      context: frontend/react_app
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    depends_on:
      - voice-app-backend
    environment:
      - REACT_APP_API_URL=http://voice-app-backend:8000
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - voice-app-frontend
      - voice-app-backend
    restart: unless-stopped
```

## ğŸ“ æ–‡æª”ç¶­è­·

### 1. API æ–‡æª”è‡ªå‹•ç”Ÿæˆ

FastAPI è‡ªå‹•ç”Ÿæˆ OpenAPI æ–‡æª”ï¼Œå¯ä»¥é€šéä»¥ä¸‹æ–¹å¼è‡ªè¨‚ï¼š

```python
# backend/api/main.py
from fastapi import FastAPI
from fastapi.openapi.docs import get_swagger_ui_html

app = FastAPI(
    title="Voice App API",
    description="Personal Voice Synthesis & Conversion Tool",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/api/v1/openapi.json"
)

@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    return get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=f"{app.title} - API Documentation",
        swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@4.15.5/swagger-ui-bundle.js",
        swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@4.15.5/swagger-ui.css",
    )
```

### 2. ç¨‹å¼ç¢¼æ–‡æª”

ä½¿ç”¨ Sphinx ç”Ÿæˆç¨‹å¼ç¢¼æ–‡æª”ï¼š

```bash
# å®‰è£ Sphinx
pip install sphinx sphinx-rtd-theme

# åˆå§‹åŒ–æ–‡æª”å°ˆæ¡ˆ
cd docs/
sphinx-quickstart

# é…ç½® conf.py
# ç”Ÿæˆæ–‡æª”
make html
```

## ğŸ” é™¤éŒ¯èˆ‡å•é¡Œæ’è§£

### 1. å¸¸è¦‹å•é¡Œè¨ºæ–·

```python
# scripts/diagnose.py
"""ç³»çµ±è¨ºæ–·è…³æœ¬"""

import torch
import platform
import subprocess
from pathlib import Path

def check_system():
    """æª¢æŸ¥ç³»çµ±ç’°å¢ƒ"""
    print("=== ç³»çµ±è³‡è¨Š ===")
    print(f"ä½œæ¥­ç³»çµ±: {platform.system()} {platform.release()}")
    print(f"Python ç‰ˆæœ¬: {platform.python_version()}")
    print(f"CPU æ ¸å¿ƒæ•¸: {os.cpu_count()}")

    # æª¢æŸ¥ GPU
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"å¯ç”¨è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("GPU: ä¸å¯ç”¨")

def check_dependencies():
    """æª¢æŸ¥ä¾è³´å¥—ä»¶"""
    required_packages = [
        "torch", "torchaudio", "transformers",
        "fastapi", "gradio", "PyQt6"
    ]

    print("\n=== å¥—ä»¶ç‰ˆæœ¬ ===")
    for package in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "æœªçŸ¥")
            print(f"{package}: {version}")
        except ImportError:
            print(f"{package}: æœªå®‰è£")

def check_models():
    """æª¢æŸ¥æ¨¡å‹ç‹€æ…‹"""
    from backend.core.config import get_settings

    settings = get_settings()
    cache_root = Path(settings.AI_CACHE_ROOT)

    print(f"\n=== æ¨¡å‹å¿«å– ({cache_root}) ===")

    if cache_root.exists():
        model_files = list(cache_root.glob("**/*.bin")) + list(cache_root.glob("**/*.pth"))
        print(f"æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹æª”æ¡ˆ")

        total_size = sum(f.stat().st_size for f in model_files) / 1e9
        print(f"ç¸½å¤§å°: {total_size:.1f} GB")
    else:
        print("å¿«å–ç›®éŒ„ä¸å­˜åœ¨")

if __name__ == "__main__":
    check_system()
    check_dependencies()
    check_models()
```

### 2. æ•ˆèƒ½åˆ†æ

```python
# scripts/profile.py
"""æ•ˆèƒ½åˆ†æè…³æœ¬"""

import time
import cProfile
import pstats
from memory_profiler import profile
from backend.services.tts_service import TTSService

@profile
def profile_tts():
    """åˆ†æ TTS æ•ˆèƒ½"""
    service = TTSService(engine="xtts", device="cuda")

    start_time = time.time()
    result = service.synthesize("é€™æ˜¯æ•ˆèƒ½æ¸¬è©¦æ–‡å­—")
    end_time = time.time()

    print(f"TTS è™•ç†æ™‚é–“: {end_time - start_time:.2f} ç§’")
    return result

def run_profiling():
    """åŸ·è¡Œæ•ˆèƒ½åˆ†æ"""
    profiler = cProfile.Profile()
    profiler.enable()

    # åŸ·è¡Œè¦åˆ†æçš„å‡½æ•¸
    profile_tts()

    profiler.disable()

    # å„²å­˜çµæœ
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # é¡¯ç¤ºå‰ 20 å€‹æœ€è€—æ™‚çš„å‡½æ•¸

    # å„²å­˜åˆ°æª”æ¡ˆ
    stats.dump_stats('performance_profile.prof')

if __name__ == "__main__":
    run_profiling()
```

---

é€šééµå¾ªä»¥ä¸Šé–‹ç™¼æŒ‡å—ï¼Œä½ å¯ä»¥å»ºç«‹é«˜å“è³ªã€å¯ç¶­è­·çš„ Voice App å°ˆæ¡ˆã€‚è¨˜ä½å§‹çµ‚éµå¾ªæœ€ä½³å¯¦è¸ï¼Œç·¨å¯«æ¸…æ™°çš„ç¨‹å¼ç¢¼ï¼Œä¸¦ä¿æŒè‰¯å¥½çš„æ¸¬è©¦è¦†è“‹ç‡ã€‚# ğŸ™ï¸ Voice App MVP - Personal Voice Synthesis & Conversion Tool

å€‹äººç”¨èªéŸ³åˆæˆèˆ‡è½‰æ›å·¥å…·ï¼Œæ”¯æ´æ–‡å­—è½‰èªéŸ³(TTS)å’ŒèªéŸ³è½‰æ›(VC)åŠŸèƒ½ã€‚æä¾›ä¸‰ç¨®å‰ç«¯ç•Œé¢ï¼šReact Webæ‡‰ç”¨ã€Gradioç ”ç©¶ç•Œé¢å’ŒPyQtæ¡Œé¢æ‡‰ç”¨ã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

- **ğŸ¤ æ–‡å­—è½‰èªéŸ³ (TTS)**: æ”¯æ´å¤šèªè¨€ã€å¤šèªªè©±è€…ã€èªé€Ÿæ§åˆ¶
- **ğŸ­ èªéŸ³è½‰æ› (VC)**: å°‡éŒ„éŸ³è½‰æ›ç‚ºä¸åŒèªªè©±è€…è²ç·š
- **ğŸ‘¤ èªªè©±è€…ç®¡ç†**: å¯è‡ªå®šç¾©èªªè©±è€…è³‡æ–™èˆ‡è²éŸ³æ¨£æœ¬
- **ğŸ“± å¤šå¹³å°æ”¯æ´**: Webã€æ¡Œé¢ã€ç ”ç©¶ç•Œé¢ä¸‰åˆä¸€
- **ğŸ’¾ é›¢ç·šå¿«å–**: æ¨¡å‹æœ¬åœ°ç·©å­˜ï¼Œæ”¯æ´é›¢ç·šä½¿ç”¨
- **ğŸ”„ å¼•æ“åˆ‡æ›**: é€éè¨­å®šæª”è¼•é¬†åˆ‡æ›TTS/VCå¼•æ“

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
voice-app/
â”œâ”€â”€ backend/                    # FastAPI å¾Œç«¯æœå‹™
â”‚   â”œâ”€â”€ api/                   # API è·¯ç”±èˆ‡ä¸­é–“ä»¶
â”‚   â”œâ”€â”€ core/                  # æ ¸å¿ƒé…ç½®èˆ‡æ¨¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ services/              # TTS/VC æœå‹™å¯¦ä½œ
â”‚   â””â”€â”€ tests/                 # API æ¸¬è©¦
â”œâ”€â”€ frontend/                   # å‰ç«¯æ‡‰ç”¨
â”‚   â”œâ”€â”€ shared/                # å…±ç”¨ API å®¢æˆ¶ç«¯
â”‚   â”œâ”€â”€ react_app/             # React Web æ‡‰ç”¨
â”‚   â”œâ”€â”€ gradio_app/            # Gradio ç ”ç©¶ç•Œé¢
â”‚   â””â”€â”€ pyqt_app/              # PyQt æ¡Œé¢æ‡‰ç”¨
â”œâ”€â”€ scripts/                   # å·¥å…·è…³æœ¬
â””â”€â”€ docs/                      # å°ˆæ¡ˆæ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒå®‰è£

```bash
# å»ºç«‹ Conda ç’°å¢ƒ
conda create -n audio-speech-lab python=3.10
conda activate audio-speech-lab

# è¨­å®šå¿«å–è·¯å¾‘ (é‡è¦!)
export AI_CACHE_ROOT=/your/preferred/cache/path

# è‡ªå‹•å®‰è£ä¾è³´èˆ‡è¨­å®š
python scripts/setup_dev.py
```

### 2. å•Ÿå‹•æœå‹™

```bash
# æ–¹æ³•ä¸€: ä¸€éµå•Ÿå‹•æ‰€æœ‰å‰ç«¯
python scripts/start_all.py

# æ–¹æ³•äºŒ: åˆ†åˆ¥å•Ÿå‹•
# å¾Œç«¯
cd backend && python -m api.main

# React (æ–°çµ‚ç«¯)
cd frontend/react_app && npm start

# Gradio (æ–°çµ‚ç«¯)
cd frontend/gradio_app && python app.py

# PyQt (æ–°çµ‚ç«¯)
cd frontend/pyqt_app && python main.py
```

### 3. è¨ªå•æ‡‰ç”¨

- **React Web**: http://localhost:3000
- **Gradio Interface**: http://localhost:7860
- **PyQt Desktop**: æ¡Œé¢æ‡‰ç”¨è¦–çª—

### 4. æ¸¬è©¦API

```bash
# å¥åº·æª¢æŸ¥
python scripts/test_api.py --quick

# å®Œæ•´æ¸¬è©¦
python scripts/test_api.py
```

## ğŸ›ï¸ é…ç½®èªªæ˜

### ç’°å¢ƒè®Šæ•¸ (.env)

```env
# è¨­å‚™èˆ‡æ•ˆèƒ½
DEVICE=cuda
FP16=true
MAX_CONCURRENCY=2

# å¼•æ“é¸æ“‡ (ä¿®æ”¹å¾Œé‡å•Ÿå³åˆ‡æ›)
TTS_ENGINE=xtts      # xtts | openvoice
VC_ENGINE=rvc        # rvc | sovits

# å¿«å–èˆ‡è·¯å¾‘ (é—œéµè¨­å®š)
AI_CACHE_ROOT=/your/ai/warehouse/path
OUTPUT_DIR=${AI_CACHE_ROOT}/outputs/voice-app
SPEAKER_DIR=${AI_CACHE_ROOT}/voice/speakers
```

### å¼•æ“åˆ‡æ›

åªéœ€ä¿®æ”¹ `.env` ä¸­çš„å¼•æ“åƒæ•¸ï¼š

```bash
# åˆ‡æ›åˆ° OpenVoice TTS
TTS_ENGINE=openvoice

# åˆ‡æ›åˆ° So-VITS-SVC èªéŸ³è½‰æ›
VC_ENGINE=sovits

# é‡å•Ÿå¾Œç«¯å³å¯ç”Ÿæ•ˆ
```

## ğŸ“¡ API ä»‹é¢

### TTS èªéŸ³åˆæˆ

```bash
curl -X POST http://localhost:8000/api/v1/tts \
  -H "Content-Type: application/json" \
  -d '{
    "text": "ä½ å¥½ï¼Œé€™æ˜¯èªéŸ³åˆæˆæ¸¬è©¦",
    "speaker_id": "default",
    "language": "zh",
    "speed": 1.0
  }'
```

### VC èªéŸ³è½‰æ›

```bash
curl -X POST http://localhost:8000/api/v1/vc \
  -H "Content-Type: application/json" \
  -d '{
    "source_audio": "base64_encoded_audio_data",
    "target_speaker": "speaker_001",
    "preserve_pitch": true
  }'
```

å®Œæ•´ API æ–‡æª”è«‹åƒé–± [docs/API.md](docs/API.md)

## ğŸ–¥ï¸ å‰ç«¯ç‰¹è‰²

### React Web æ‡‰ç”¨
- ç¾ä»£åŒ–éŸ¿æ‡‰å¼ç•Œé¢
- æ‹–æ”¾ä¸Šå‚³æ–‡ä»¶
- å³æ™‚éŸ³é »é è¦½
- åŸç”Ÿ CSS æ¨£å¼ç³»çµ±

### Gradio ç ”ç©¶ç•Œé¢
- AI ç ”ç©¶äººå“¡å‹å¥½
- å¿«é€ŸåŸå‹æ¸¬è©¦
- è‡ªå‹•ç”Ÿæˆç•Œé¢
- æ”¯æ´æ‰¹é‡è™•ç†

### PyQt æ¡Œé¢æ‡‰ç”¨
- åŸç”Ÿæ¡Œé¢é«”é©—
- é›¢ç·šæ¨¡å‹å¿«å–
- æ‹–æ”¾æ‰¹é‡è™•ç†
- æ·±è‰²/æ·ºè‰²ä¸»é¡Œ

## ğŸ”§ é–‹ç™¼æŒ‡å—

### å°ˆæ¡ˆçµæ§‹èªªæ˜

- `backend/api/`: FastAPI è·¯ç”±èˆ‡ä¸­é–“ä»¶
- `backend/core/`: æ ¸å¿ƒé…ç½®ã€æ¨¡å‹ç®¡ç†ã€æ€§èƒ½å„ªåŒ–
- `backend/services/`: TTS/VC å¼•æ“å°è£
- `frontend/shared/`: è·¨å¹³å° API å®¢æˆ¶ç«¯
- `scripts/`: é–‹ç™¼å·¥å…·èˆ‡æ¸¬è©¦è…³æœ¬

### Git å·¥ä½œæµç¨‹

```bash
# å‰µå»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/new-tts-engine

# æäº¤è®Šæ›´ (ä½¿ç”¨ Conventional Commits)
git commit -m "feat(tts): add OpenVoice engine support"
git commit -m "fix(vc): resolve pitch preservation issue"
git commit -m "docs(readme): update setup instructions"

# åˆä½µåˆ°ä¸»åˆ†æ”¯
git checkout develop
git merge --no-ff feature/new-tts-engine
```

### æ¸¬è©¦ç­–ç•¥

```bash
# å–®å…ƒæ¸¬è©¦
cd backend && python -m pytest tests/

# API æ•´åˆæ¸¬è©¦
python scripts/test_api.py

# å‰ç«¯æ¸¬è©¦
cd frontend/react_app && npm test
```

## ğŸ“š æ–‡æª”èˆ‡è³‡æº

- [ğŸ“– è¨­å®šæŒ‡å—](docs/SETUP.md) - è©³ç´°å®‰è£èˆ‡é…ç½®èªªæ˜
- [ğŸ”Œ API æ–‡æª”](docs/API.md) - å®Œæ•´ API åƒè€ƒ
- [ğŸ‘¨â€ğŸ’» é–‹ç™¼æŒ‡å—](docs/DEVELOPMENT.md) - é–‹ç™¼å·¥ä½œæµç¨‹èˆ‡æœ€ä½³å¯¦è¸

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

**å•é¡Œ: å¾Œç«¯å•Ÿå‹•å¤±æ•—**
```bash
# æª¢æŸ¥ä¾è³´æ˜¯å¦å®‰è£å®Œæ•´
pip install -r backend/requirements.txt

# æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"
```

**å•é¡Œ: æ¨¡å‹ä¸‹è¼‰å¤±æ•—**
```bash
# æª¢æŸ¥å¿«å–ç›®éŒ„æ¬Šé™
ls -la $AI_CACHE_ROOT

# æ‰‹å‹•å‰µå»ºç›®éŒ„
python scripts/download_models.py --setup
```

**å•é¡Œ: å‰ç«¯é€£ç·šå¤±æ•—**
```bash
# æª¢æŸ¥å¾Œç«¯å¥åº·ç‹€æ…‹
curl http://localhost:8000/healthz

# æª¢æŸ¥é˜²ç«ç‰†è¨­å®š
netstat -an | grep :8000
```

### æ•ˆèƒ½å„ªåŒ–

- **VRAM ä¸è¶³**: èª¿æ•´ `MAX_CONCURRENCY=1` æˆ–ä½¿ç”¨ CPU æ¨¡å¼
- **è™•ç†é€Ÿåº¦æ…¢**: ç¢ºä¿ GPU é©…å‹•èˆ‡ CUDA ç‰ˆæœ¬åŒ¹é…
- **éŸ³è³ªå•é¡Œ**: æª¢æŸ¥ `LUFS` æ­£è¦åŒ–è¨­å®š


## ğŸ“„ æˆæ¬Šæ¢æ¬¾

MIT License - è©³è¦‹ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è¬

- [XTTS](https://github.com/coqui-ai/TTS) - é«˜å“è³ªæ–‡å­—è½‰èªéŸ³
- [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) - èªéŸ³è½‰æ›æŠ€è¡“
- [FastAPI](https://fastapi.tiangolo.com/) - ç¾ä»£ Python Web æ¡†æ¶
- [React](https://reactjs.org/) - å‰ç«¯é–‹ç™¼æ¡†æ¶
- [Gradio](https://gradio.app/) - AI æ¨¡å‹ç•Œé¢ç”Ÿæˆ
- [PyQt](https://www.riverbankcomputing.com/software/pyqt/) - è·¨å¹³å°æ¡Œé¢æ‡‰ç”¨æ¡†æ¶

---

**Voice App MVP** - è®“èªéŸ³åˆæˆè®Šå¾—ç°¡å–®æ˜“ç”¨ï¼ ğŸ¤âœ¨

---

# docs/SETUP.md - è©³ç´°å®‰è£æŒ‡å—

## ğŸ› ï¸ ç³»çµ±éœ€æ±‚

### æœ€ä½éœ€æ±‚
- **ä½œæ¥­ç³»çµ±**: Windows 10+, macOS 10.14+, Linux Ubuntu 18.04+
- **Python**: 3.8+ (å»ºè­° 3.10)
- **è¨˜æ†¶é«”**: 8GB RAM (å»ºè­° 16GB+)
- **å„²å­˜ç©ºé–“**: 10GB (æ¨¡å‹å¿«å–éœ€é¡å¤–ç©ºé–“)

### æ¨è–¦éœ€æ±‚
- **GPU**: NVIDIA RTX 3060+ (8GB+ VRAM)
- **CPU**: 8æ ¸å¿ƒä»¥ä¸Š
- **è¨˜æ†¶é«”**: 32GB RAM
- **å„²å­˜**: SSD ç¡¬ç¢Ÿ

### è»Ÿé«”ä¾è³´
- **Node.js**: 14+ (React å‰ç«¯)
- **FFmpeg**: éŸ³è¨Šè™•ç†
- **Git**: ç‰ˆæœ¬æ§åˆ¶
- **CUDA**: 11.8+ (GPU åŠ é€Ÿ)

