# backend/services/vc_service.py

"""
èªéŸ³è½‰æ› (VC) æœå‹™
"""
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional
import torch
import numpy as np
import librosa
import soundfile as sf

from backend.core.config import Settings
from backend.core.audio.normalization import quick_normalize
from backend.core.audio.io import load_audio, trim_silence


class VCService:
    """VC æœå‹™å°è£"""

    def __init__(self, model: Dict[str, Any], settings: Settings):
        """
        åˆå§‹åŒ– VC æœå‹™

        Args:
            model: è¼‰å…¥çš„ VC æ¨¡å‹
            settings: æ‡‰ç”¨è¨­å®š
        """
        self.model = model
        self.settings = settings
        self.engine_type = model.get("type", "unknown")

        print(f"ğŸ­ VC æœå‹™åˆå§‹åŒ–: {self.engine_type}")

    async def convert(
        self,
        source_audio_path: str,
        target_speaker: str,
        output_path: str,
        preserve_pitch: bool = True,
        denoise: bool = True,
        f0_method: str = "harvest",
    ) -> Dict[str, Any]:
        """
        åŸ·è¡ŒèªéŸ³è½‰æ›

        Args:
            source_audio_path: æºéŸ³æª”è·¯å¾‘
            target_speaker: ç›®æ¨™èªªè©±è€… ID
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            preserve_pitch: æ˜¯å¦ä¿æŒéŸ³èª¿
            denoise: æ˜¯å¦é™å™ª
            f0_method: åŸºé »æå–æ–¹æ³•

        Returns:
            è½‰æ›çµæœè³‡è¨Š
        """
        start_time = time.time()

        try:
            print(f"ğŸ¯ VC è½‰æ›é–‹å§‹: {self.engine_type}")
            print(f"   æºéŸ³æª”: {source_audio_path}")
            print(f"   ç›®æ¨™èªªè©±è€…: {target_speaker}")
            print(f"   è¨­å®š: pitch={preserve_pitch}, denoise={denoise}, f0={f0_method}")

            # è¼‰å…¥æºéŸ³æª”
            source_audio, sample_rate = load_audio(
                source_audio_path, self.settings.SAMPLE_RATE
            )
            original_duration = len(source_audio) / sample_rate

            # é è™•ç†
            if denoise:
                source_audio = await self._denoise_audio(source_audio, sample_rate)

            # æ ¹æ“šå¼•æ“é¡å‹åŸ·è¡Œè½‰æ›
            if self.engine_type == "rvc":
                converted_audio = await self._convert_rvc(
                    source_audio, target_speaker, preserve_pitch, f0_method
                )
            elif self.engine_type == "sovits":
                converted_audio = await self._convert_sovits(
                    source_audio, target_speaker, preserve_pitch, f0_method
                )
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„ VC å¼•æ“: {self.engine_type}")

            # å¾Œè™•ç†
            converted_audio = await self._post_process_audio(
                converted_audio, sample_rate
            )

            # å„²å­˜éŸ³æª”
            sf.write(output_path, converted_audio, sample_rate, format="WAV")

            processing_time = time.time() - start_time
            output_duration = len(converted_audio) / sample_rate

            print(
                f"âœ… VC è½‰æ›å®Œæˆ: {original_duration:.2f}s -> {output_duration:.2f}sï¼Œè€—æ™‚ {processing_time:.2f}s"
            )
            print(f"ğŸ’¾ éŸ³æª”å·²å„²å­˜: {output_path}")

            return {
                "sample_rate": sample_rate,
                "processing_time": processing_time,
                "engine": self.engine_type,
                "original_duration": original_duration,
                "output_duration": output_duration,
                "settings": {
                    "preserve_pitch": preserve_pitch,
                    "denoise": denoise,
                    "f0_method": f0_method,
                },
            }

        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ VC è½‰æ›å¤±æ•—: {e} (è€—æ™‚ {processing_time:.2f}s)")
            raise

    async def _convert_rvc(
        self,
        audio_data: np.ndarray,
        target_speaker: str,
        preserve_pitch: bool,
        f0_method: str,
    ) -> np.ndarray:
        """RVC å¼•æ“è½‰æ› (å¯¦ä½œä½”ä½ç¬¦)"""
        await asyncio.sleep(0.5)  # æ¨¡æ“¬è™•ç†æ™‚é–“

        print(f"ğŸ”„ RVC è½‰æ›ä¸­: ç›®æ¨™èªªè©±è€… {target_speaker}")

        # RVC æ¨¡æ“¬å¯¦ä½œ
        # å¯¦éš›æ‡‰è©²è¼‰å…¥ RVC æ¨¡å‹ä¸¦é€²è¡ŒèªéŸ³è½‰æ›

        # æ¨¡æ“¬éŸ³è‰²è½‰æ›ï¼šèª¿æ•´é »è­œç‰¹å¾µ
        converted_audio = audio_data.copy()

        # æ¨¡æ“¬éŸ³èª¿ä¿æŒ/è®ŠåŒ–
        if preserve_pitch:
            # ä¿æŒåŸå§‹éŸ³èª¿ï¼Œåªæ”¹è®ŠéŸ³è‰²
            pitch_shift = 0.0
        else:
            # æ ¹æ“šç›®æ¨™èªªè©±è€…èª¿æ•´éŸ³èª¿
            speaker_pitch_map = {
                "default": 0.0,
                "speaker_001": 0.05,
                "speaker_002": -0.05,
                "speaker_003": 0.1,
            }
            pitch_shift = speaker_pitch_map.get(target_speaker, 0.0)

        # æ‡‰ç”¨éŸ³èª¿åç§» (ç°¡åŒ–å¯¦ä½œ)
        if pitch_shift != 0.0:
            converted_audio = librosa.effects.pitch_shift(
                converted_audio,
                sr=self.settings.SAMPLE_RATE,
                n_steps=pitch_shift * 12,  # è½‰æ›ç‚ºåŠéŸ³
            )

        # æ¨¡æ“¬éŸ³è‰²è®ŠåŒ–ï¼šèª¿æ•´è«§æ³¢çµæ§‹
        # å¯¦éš› RVC æœƒä½¿ç”¨ VAE å’Œ flow-based æ¨¡å‹
        harmonic_shift = np.random.uniform(0.9, 1.1)

        # ç°¡å–®çš„é »åŸŸè™•ç†æ¨¡æ“¬
        stft = librosa.stft(converted_audio)
        magnitude = np.abs(stft)
        phase = np.angle(stft)

        # èª¿æ•´é »è­œåŒ…çµ¡ (æ¨¡æ“¬è²é“ç‰¹å¾µè®ŠåŒ–)
        for i in range(magnitude.shape[0]):
            magnitude[i, :] *= 0.8 + 0.4 * np.random.random()

        # é‡æ§‹éŸ³è¨Š
        modified_stft = magnitude * np.exp(1j * phase)
        converted_audio = librosa.istft(modified_stft)

        # ç¢ºä¿é•·åº¦ä¸€è‡´
        if len(converted_audio) != len(audio_data):
            if len(converted_audio) > len(audio_data):
                converted_audio = converted_audio[: len(audio_data)]
            else:
                # å¡«è£œé•·åº¦å·®ç•°
                pad_length = len(audio_data) - len(converted_audio)
                converted_audio = np.pad(
                    converted_audio, (0, pad_length), mode="constant"
                )

        print(f"âœ¨ RVC è½‰æ›å®Œæˆ: éŸ³èª¿åç§» {pitch_shift:.2f}")

        return converted_audio

    async def _convert_sovits(
        self,
        audio_data: np.ndarray,
        target_speaker: str,
        preserve_pitch: bool,
        f0_method: str,
    ) -> np.ndarray:
        """So-VITS-SVC å¼•æ“è½‰æ› (å¯¦ä½œä½”ä½ç¬¦)"""
        await asyncio.sleep(0.8)  # SoVITS è™•ç†æ™‚é–“è¼ƒé•·

        print(f"ğŸ”„ SoVITS è½‰æ›ä¸­: ç›®æ¨™èªªè©±è€… {target_speaker}, F0æ–¹æ³• {f0_method}")

        # So-VITS-SVC æ¨¡æ“¬å¯¦ä½œ
        converted_audio = audio_data.copy()

        # F0 æå–æ–¹æ³•æ¨¡æ“¬
        f0_methods = {
            "harvest": 1.0,  # è¼ƒç©©å®š
            "dio": 1.1,  # è¼ƒå¿«ä½†å¯èƒ½æœ‰é›œè¨Š
            "crepe": 0.95,  # æœ€æº–ç¢ºä½†è¼ƒæ…¢
            "parselmouth": 1.05,  # å¹³è¡¡é¸æ“‡
        }

        f0_factor = f0_methods.get(f0_method, 1.0)

        # æ¨¡æ“¬é«˜å“è³ªèªéŸ³è½‰æ›
        # å¯¦éš› SoVITS ä½¿ç”¨ diffusion model å’Œ vector quantization

        # éŸ³èª¿è™•ç†
        if preserve_pitch:
            pitch_variation = np.random.uniform(0.98, 1.02)  # å¾®å°è®ŠåŒ–
        else:
            # ç›®æ¨™èªªè©±è€…éŸ³èª¿ç‰¹å¾µ
            speaker_f0_map = {
                "default": 1.0,
                "speaker_001": 1.15,  # è¼ƒé«˜éŸ³
                "speaker_002": 0.85,  # è¼ƒä½éŸ³
                "speaker_003": 1.25,  # é«˜éŸ³
            }
            pitch_variation = speaker_f0_map.get(target_speaker, 1.0)

        # æ‡‰ç”¨éŸ³èª¿è®ŠåŒ–
        if pitch_variation != 1.0:
            converted_audio = librosa.effects.pitch_shift(
                converted_audio,
                sr=self.settings.SAMPLE_RATE,
                n_steps=12 * np.log2(pitch_variation * f0_factor),
            )

        # æ¨¡æ“¬æ›´ç²¾ç´°çš„è²å­¸ç‰¹å¾µè½‰æ›
        # æ·»åŠ ç´°å¾®çš„å…±æŒ¯å³°èª¿æ•´
        stft = librosa.stft(converted_audio, n_fft=2048, hop_length=512)
        magnitude = np.abs(stft)
        phase = np.angle(stft)

        # æ¨¡æ“¬è²é“é•·åº¦è®ŠåŒ– (æ”¹è®Šå…±æŒ¯å³°)
        vocal_tract_scaling = np.random.uniform(0.9, 1.1)

        # é »è­œåŒ…çµ¡èª¿æ•´ (æ¨¡æ“¬ä¸åŒèªªè©±è€…çš„è²é“ç‰¹å¾µ)
        freq_bins = magnitude.shape[0]
        for i in range(freq_bins):
            # æ¨¡æ“¬å…±æŒ¯å³°åç§»
            formant_factor = (
                1.0 + 0.1 * np.sin(i / freq_bins * 4 * np.pi) * vocal_tract_scaling
            )
            magnitude[i, :] *= formant_factor

        # æ·»åŠ èªªè©±è€…ç‰¹æœ‰çš„é »è­œç‰¹å¾µ
        speaker_coloring = {
            "default": 1.0,
            "speaker_001": [1.1, 0.9, 1.2],  # å¼·åŒ–é«˜é »
            "speaker_002": [0.9, 1.1, 0.8],  # å¼·åŒ–ä¸­é »
            "speaker_003": [1.2, 1.0, 1.1],  # å‡è¡¡å¢å¼·
        }

        if target_speaker in speaker_coloring and target_speaker != "default":
            coloring = speaker_coloring[target_speaker]
            # åˆ†é »æ®µèª¿æ•´
            low_end = freq_bins // 3
            mid_end = 2 * freq_bins // 3

            magnitude[:low_end, :] *= coloring[0]  # ä½é »
            magnitude[low_end:mid_end, :] *= coloring[1]  # ä¸­é »
            magnitude[mid_end:, :] *= coloring[2]  # é«˜é »

        # é‡æ§‹éŸ³è¨Š
        modified_stft = magnitude * np.exp(1j * phase)
        converted_audio = librosa.istft(modified_stft, hop_length=512)

        # é•·åº¦å°é½Š
        if len(converted_audio) != len(audio_data):
            if len(converted_audio) > len(audio_data):
                converted_audio = converted_audio[: len(audio_data)]
            else:
                pad_length = len(audio_data) - len(converted_audio)
                converted_audio = np.pad(
                    converted_audio, (0, pad_length), mode="constant"
                )

        # æ·»åŠ ç´°å¾®çš„å‹•æ…‹ç¯„åœèª¿æ•´
        dynamic_factor = np.random.uniform(0.95, 1.05)
        converted_audio *= dynamic_factor

        print(
            f"âœ¨ SoVITS è½‰æ›å®Œæˆ: éŸ³èª¿è®ŠåŒ– {pitch_variation:.2f}x, F0å› å­ {f0_factor:.2f}"
        )

        return converted_audio

    async def _denoise_audio(
        self, audio_data: np.ndarray, sample_rate: int
    ) -> np.ndarray:
        """éŸ³è¨Šé™å™ªè™•ç† (ç°¡åŒ–å¯¦ä½œ)"""
        try:
            # ç°¡å–®çš„é™å™ªï¼šé«˜é€šæ¿¾æ³¢ + å‹•æ…‹ç¯„åœå£“ç¸®

            # 1. é«˜é€šæ¿¾æ³¢å»é™¤ä½é »å™ªéŸ³
            from scipy.signal import butter, filtfilt

            nyquist = sample_rate / 2
            low_cutoff = 80 / nyquist  # 80Hz é«˜é€š

            b, a = butter(2, low_cutoff, btype="high")
            denoised_audio = filtfilt(b, a, audio_data)

            # 2. å‹•æ…‹ç¯„åœè™•ç†
            # è¨ˆç®—éŸ³è¨ŠåŒ…çµ¡
            window_size = int(sample_rate * 0.02)  # 20ms çª—å£
            envelope = np.convolve(
                np.abs(denoised_audio), np.ones(window_size) / window_size, mode="same"
            )

            # å™ªéŸ³é–¾å€¼
            noise_threshold = np.percentile(envelope, 20)  # åº•éƒ¨ 20% è¦–ç‚ºå™ªéŸ³

            # è»Ÿæ€§å™ªéŸ³é–€é™
            gate_ratio = np.clip((envelope - noise_threshold) / noise_threshold, 0, 1)
            gate_ratio = gate_ratio**0.5  # å¹³æ»‘éæ¸¡

            denoised_audio = denoised_audio * gate_ratio

            print(f"ğŸ”‡ é™å™ªå®Œæˆ: å™ªéŸ³é–¾å€¼ {noise_threshold:.4f}")

            return denoised_audio

        except Exception as e:
            print(f"âš ï¸ é™å™ªå¤±æ•—: {e}")
            return audio_data

    async def _post_process_audio(
        self, audio_data: np.ndarray, sample_rate: int
    ) -> np.ndarray:
        """éŸ³è¨Šå¾Œè™•ç†"""
        try:
            # å»é™¤å‰å¾ŒéœéŸ³
            audio_data = trim_silence(audio_data, sample_rate, top_db=30)

            # LUFS æ­£è¦åŒ–
            audio_data = quick_normalize(
                audio_data, self.settings.TARGET_LUFS, sample_rate
            )

            # è»Ÿé™åˆ¶å™¨é˜²æ­¢å‰Šæ³¢
            peak = np.max(np.abs(audio_data))
            if peak > 0.95:
                # è»Ÿå‰Šæ³¢è€Œéç¡¬é™åˆ¶
                audio_data = np.tanh(audio_data / peak * 0.95) * 0.95

            # æ·»åŠ å¾®å°çš„æŠ–å‹•æ¸›å°‘é‡åŒ–å™ªéŸ³
            dither = np.random.normal(0, 1e-5, len(audio_data))
            audio_data += dither

            print(
                f"ğŸšï¸ å¾Œè™•ç†å®Œæˆ: å³°å€¼ {peak:.3f}, é•·åº¦ {len(audio_data)/sample_rate:.2f}s"
            )

            return audio_data

        except Exception as e:
            print(f"âš ï¸ éŸ³è¨Šå¾Œè™•ç†å¤±æ•—: {e}")
            return audio_data

    def get_available_speakers(self) -> list:
        """å–å¾—å¯ç”¨èªªè©±è€…åˆ—è¡¨"""
        try:
            speakers_dir = Path(self.settings.SPEAKER_DIR)
            speakers = ["default"]

            if speakers_dir.exists():
                for speaker_file in speakers_dir.glob("*.json"):
                    speakers.append(speaker_file.stem)

            # æ·»åŠ å¼•æ“ç‰¹å®šçš„é è¨­èªªè©±è€…
            engine_speakers = {
                "rvc": ["speaker_001", "speaker_002", "speaker_003"],
                "sovits": ["voice_a", "voice_b", "voice_c", "voice_d"],
            }

            if self.engine_type in engine_speakers:
                speakers.extend(engine_speakers[self.engine_type])

            return list(set(speakers))  # å»é‡

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è¼‰å…¥èªªè©±è€…åˆ—è¡¨: {e}")
            return ["default"]

    def get_supported_f0_methods(self) -> list:
        """å–å¾—æ”¯æ´çš„ F0 æå–æ–¹æ³•"""
        method_map = {
            "rvc": ["harvest", "dio", "crepe", "parselmouth"],
            "sovits": ["harvest", "dio", "crepe"],
        }

        return method_map.get(self.engine_type, ["harvest"])

    def estimate_processing_time(self, audio_duration: float) -> float:
        """ä¼°ç®—è™•ç†æ™‚é–“"""
        # æ ¹æ“šå¼•æ“å’ŒéŸ³è¨Šé•·åº¦ä¼°ç®—
        time_factors = {"rvc": 2.0, "sovits": 4.0}  # RVC è¼ƒå¿«  # SoVITS è¼ƒæ…¢ä½†å“è³ªå¥½

        base_factor = time_factors.get(self.engine_type, 3.0)

        # GPU åŠ é€Ÿ
        if torch.cuda.is_available():
            base_factor *= 0.3

        return audio_duration * base_factor
