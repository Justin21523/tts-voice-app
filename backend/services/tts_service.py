# backend/services/tts_service.py

"""
æ–‡å­—è½‰èªéŸ³ (TTS) æœå‹™
"""
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional

import numpy as np
import torch
import librosa
import soundfile as sf

from backend.core.config import Settings
from backend.core.audio.normalization import quick_normalize
from backend.core.audio.io import trim_silence


class TTSService:
    """TTS æœå‹™å°è£"""

    def __init__(self, model: Dict[str, Any], settings: Settings):
        """
        åˆå§‹åŒ– TTS æœå‹™

        Args:
            model: è¼‰å…¥çš„ TTS æ¨¡å‹
            settings: æ‡‰ç”¨è¨­å®š
        """
        self.model = model
        self.settings = settings
        self.engine_type = model.get("type", "unknown")

        print(f"ğŸ¤ TTS æœå‹™åˆå§‹åŒ–: {self.engine_type}")

    async def synthesize(
        self,
        text: str,
        speaker_id: str = "default",
        language: str = "zh",
        speed: float = 1.0,
        emotion: str = "neutral",
        output_path: str = None,
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ–‡å­—è½‰èªéŸ³

        Args:
            text: è¦åˆæˆçš„æ–‡å­—
            speaker_id: èªªè©±è€… ID
            language: èªè¨€ä»£ç¢¼
            speed: èªé€Ÿå€ç‡
            emotion: æƒ…æ„Ÿé¢¨æ ¼
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

        Returns:
            åˆæˆçµæœè³‡è¨Š
        """
        start_time = time.time()

        try:
            print(f"ğŸ¯ TTS åˆæˆé–‹å§‹: {self.engine_type}")
            print(f"   æ–‡å­—: {text[:50]}{'...' if len(text) > 50 else ''}")
            print(f"   èªªè©±è€…: {speaker_id}")
            print(f"   èªè¨€: {language}, èªé€Ÿ: {speed}x")

            # æ ¹æ“šå¼•æ“é¡å‹åŸ·è¡Œåˆæˆ
            if self.engine_type == "xtts":
                audio_data, sample_rate = await self._synthesize_xtts(
                    text, speaker_id, language, speed, emotion
                )
            elif self.engine_type == "openvoice":
                audio_data, sample_rate = await self._synthesize_openvoice(
                    text, speaker_id, language, speed, emotion
                )
            elif self.engine_type == "bark":
                audio_data, sample_rate = await self._synthesize_bark(
                    text, speaker_id, language, speed, emotion
                )
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„ TTS å¼•æ“: {self.engine_type}")

            # å¾Œè™•ç†
            audio_data = await self._post_process_audio(audio_data, sample_rate)

            # å„²å­˜éŸ³æª”
            if output_path:
                sf.write(output_path, audio_data, sample_rate, format="WAV")
                print(f"ğŸ’¾ éŸ³æª”å·²å„²å­˜: {output_path}")

            processing_time = time.time() - start_time
            duration = len(audio_data) / sample_rate

            print(f"âœ… TTS åˆæˆå®Œæˆ: {duration:.2f}s éŸ³æª”ï¼Œè€—æ™‚ {processing_time:.2f}s")

            return {
                "duration": duration,
                "sample_rate": sample_rate,
                "processing_time": processing_time,
                "engine": self.engine_type,
                "text_length": len(text),
            }

        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ TTS åˆæˆå¤±æ•—: {e} (è€—æ™‚ {processing_time:.2f}s)")
            raise

    async def _synthesize_xtts(
        self, text: str, speaker_id: str, language: str, speed: float, emotion: str
    ) -> tuple:
        """XTTS å¼•æ“åˆæˆ (å¯¦ä½œä½”ä½ç¬¦)"""
        # é€™è£¡æ‡‰è©²å¯¦ä½œå¯¦éš›çš„ XTTS å‘¼å«
        # æš«æ™‚ç”¢ç”Ÿæ¨¡æ“¬éŸ³è¨Š

        await asyncio.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“

        # ç”¢ç”Ÿæ¨¡æ“¬éŸ³è¨Š (440Hz æ­£å¼¦æ³¢ï¼ŒæŒçºŒæ™‚é–“ä¾æ–‡å­—é•·åº¦)
        duration = max(1.0, len(text) * 0.05)  # æ–‡å­—é•·åº¦å½±éŸ¿éŸ³æª”é•·åº¦
        sample_rate = self.settings.SAMPLE_RATE

        t = np.linspace(0, duration, int(sample_rate * duration), False)
        frequency = 440.0  # A4 éŸ³ç¬¦

        # æ ¹æ“šèªé€Ÿèª¿æ•´
        frequency = frequency * (1.1 if speed > 1.0 else 0.9 if speed < 1.0 else 1.0)

        # ç”¢ç”Ÿæ­£å¼¦æ³¢ + å°‘é‡å™ªéŸ³
        audio_data = 0.3 * np.sin(frequency * 2 * np.pi * t)
        audio_data += 0.05 * np.random.normal(0, 1, len(audio_data))  # åŠ å…¥å™ªéŸ³

        # æ·»åŠ åŒ…çµ¡ç·šä½¿è²éŸ³æ›´è‡ªç„¶
        envelope = np.ones_like(audio_data)
        fade_samples = int(sample_rate * 0.05)  # 50ms fade
        envelope[:fade_samples] = np.linspace(0, 1, fade_samples)
        envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)
        audio_data *= envelope

        print(f"ğŸ”Š XTTS æ¨¡æ“¬åˆæˆ: {duration:.2f}s, {sample_rate}Hz")

        return audio_data, sample_rate

    async def _synthesize_openvoice(
        self, text: str, speaker_id: str, language: str, speed: float, emotion: str
    ) -> tuple:
        """OpenVoice å¼•æ“åˆæˆ (å¯¦ä½œä½”ä½ç¬¦)"""
        await asyncio.sleep(0.1)

        # OpenVoice æ¨¡æ“¬å¯¦ä½œ
        duration = max(1.0, len(text) * 0.06)
        sample_rate = self.settings.SAMPLE_RATE

        t = np.linspace(0, duration, int(sample_rate * duration), False)

        # ç”¢ç”Ÿæ›´è¤‡é›œçš„åˆæˆéŸ³ (å¤šå€‹é »ç‡çµ„åˆ)
        frequencies = [220, 440, 880]  # åŸºé »å’Œè«§æ³¢
        audio_data = np.zeros(len(t))

        for i, freq in enumerate(frequencies):
            amplitude = 0.2 / (i + 1)  # è«§æ³¢è¡°æ¸›
            audio_data += amplitude * np.sin(freq * speed * 2 * np.pi * t)

        # æ·»åŠ åŒ…çµ¡ç·šå’Œå°‘é‡å™ªéŸ³
        envelope = 1 - 0.3 * np.random.random(len(audio_data))
        audio_data *= envelope
        audio_data += 0.02 * np.random.normal(0, 1, len(audio_data))

        print(f"ğŸ”Š OpenVoice æ¨¡æ“¬åˆæˆ: {duration:.2f}s")

        return audio_data, sample_rate

    async def _synthesize_bark(
        self, text: str, speaker_id: str, language: str, speed: float, emotion: str
    ) -> tuple:
        """Bark å¼•æ“åˆæˆ (å¯¦ä½œä½”ä½ç¬¦)"""
        await asyncio.sleep(0.2)

        # Bark æ¨¡æ“¬å¯¦ä½œ (æ›´æœ‰å‰µæ„çš„åˆæˆ)
        duration = max(1.5, len(text) * 0.08)
        sample_rate = self.settings.SAMPLE_RATE

        t = np.linspace(0, duration, int(sample_rate * duration), False)

        # ç”¢ç”Ÿé¡ä¼¼èªéŸ³çš„é »è­œ
        formants = [800, 1200, 2400]  # èªéŸ³å…±æŒ¯å³°
        audio_data = np.zeros(len(t))

        for formant in formants:
            # éš¨æ©Ÿèª¿è£½é »ç‡
            modulation = 1 + 0.1 * np.sin(5 * 2 * np.pi * t)
            freq = formant * speed * modulation

            amplitude = 0.15 * np.random.random()
            audio_data += amplitude * np.sin(freq * 2 * np.pi * t)

        # æ·»åŠ èªéŸ³ç‰¹æœ‰çš„æ™‚è®ŠåŒ…çµ¡
        segments = int(duration * 3)  # æ¯ç§’ 3 å€‹éŸ³æ®µ
        for i in range(segments):
            start_idx = int(i * len(audio_data) / segments)
            end_idx = int((i + 1) * len(audio_data) / segments)

            segment_envelope = np.random.random() * 0.8 + 0.2
            audio_data[start_idx:end_idx] *= segment_envelope

        print(f"ğŸ”Š Bark æ¨¡æ“¬åˆæˆ: {duration:.2f}s (å‰µæ„æ¨¡å¼)")

        return audio_data, sample_rate

    async def _post_process_audio(
        self, audio_data: np.ndarray, sample_rate: int
    ) -> np.ndarray:
        """éŸ³è¨Šå¾Œè™•ç†"""
        try:
            # å»é™¤å‰å¾ŒéœéŸ³
            audio_data = trim_silence(audio_data, sample_rate, top_db=25)

            # LUFS æ­£è¦åŒ–
            audio_data = quick_normalize(
                audio_data, self.settings.TARGET_LUFS, sample_rate
            )

            # é™åˆ¶å³°å€¼é¿å…å‰Šæ³¢
            peak = np.max(np.abs(audio_data))
            if peak > 0.95:
                audio_data = audio_data / peak * 0.95

            print(
                f"ğŸšï¸ å¾Œè™•ç†å®Œæˆ: å³°å€¼ {peak:.3f}, é•·åº¦ {len(audio_data)/sample_rate:.2f}s"
            )

            return audio_data

        except Exception as e:
            print(f"âš ï¸ éŸ³è¨Šå¾Œè™•ç†å¤±æ•—: {e}")
            return audio_data

    def get_available_speakers(self) -> list:
        """å–å¾—å¯ç”¨èªªè©±è€…åˆ—è¡¨"""
        try:
            speakers_dir = Path(self.settings.SPEAKER_DIR)
            speakers = ["default"]

            if speakers_dir.exists():
                for speaker_file in speakers_dir.glob("*.json"):
                    speakers.append(speaker_file.stem)

            return speakers

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è¼‰å…¥èªªè©±è€…åˆ—è¡¨: {e}")
            return ["default"]

    def get_supported_languages(self) -> list:
        """å–å¾—æ”¯æ´çš„èªè¨€åˆ—è¡¨"""
        # æ ¹æ“šå¼•æ“è¿”å›æ”¯æ´çš„èªè¨€
        language_map = {
            "xtts": ["zh", "en", "ja", "ko", "es", "fr", "de", "it"],
            "openvoice": ["zh", "en", "ja", "ko"],
            "bark": ["zh", "en", "multilingual"],
        }

        return language_map.get(self.engine_type, ["zh", "en"])
