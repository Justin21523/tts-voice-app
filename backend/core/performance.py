# backend/core/performance.py

"""
æ•ˆèƒ½æœ€ä½³åŒ–è¨­å®š
"""
import torch
import warnings
from backend.core.config import Settings


def setup_performance_defaults(settings: Settings):
    """è¨­å®šæ•ˆèƒ½é è¨­å€¼"""

    # å¿½ç•¥ä¸é‡è¦çš„è­¦å‘Š
    warnings.filterwarnings("ignore", category=FutureWarning)
    warnings.filterwarnings("ignore", category=UserWarning, module="torch")

    # PyTorch æœ€ä½³åŒ–
    if torch.cuda.is_available():
        # å•Ÿç”¨ cuDNN benchmark ä»¥æœ€ä½³åŒ– convolution æ•ˆèƒ½
        torch.backends.cudnn.benchmark = True

        # è¨­å®šæ··åˆç²¾åº¦é è¨­
        if settings.FP16:
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cuda.matmul.allow_tf32 = True
            print("âœ… å·²å•Ÿç”¨ TF32 æ··åˆç²¾åº¦")

        # è¨­å®šè¨˜æ†¶é«”åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()

        print(f"ğŸš€ CUDA æ•ˆèƒ½æœ€ä½³åŒ–å·²å•Ÿç”¨")
        print(f"   è£ç½®æ•¸é‡: {torch.cuda.device_count()}")
        print(f"   ç•¶å‰è£ç½®: {torch.cuda.get_device_name()}")
        print(
            f"   è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )

    # CPU æœ€ä½³åŒ–
    if hasattr(torch, "set_num_threads"):
        # è¨­å®š CPU åŸ·è¡Œç·’æ•¸ï¼ˆæ ¹æ“š CPU æ ¸å¿ƒæ•¸èª¿æ•´ï¼‰
        import os

        cpu_count = os.cpu_count() or 4
        torch.set_num_threads(min(cpu_count, 8))  # é™åˆ¶æœ€å¤§ 8 åŸ·è¡Œç·’
        print(f"ğŸ–¥ï¸ CPU åŸ·è¡Œç·’æ•¸: {torch.get_num_threads()}")

    # è¨­å®šæµ®é»æ•¸ç²¾åº¦
    if settings.FP16:
        torch.set_float32_matmul_precision("medium")
        print("âœ… æ··åˆç²¾åº¦ (FP16) å·²å•Ÿç”¨")


def enable_attention_slicing():
    """å•Ÿç”¨ attention slicing ä»¥ç¯€çœ VRAM (Diffusion æ¨¡å‹ç”¨)"""
    try:
        # é€™å€‹å‡½æ•¸åœ¨å¯¦éš›æ•´åˆ Stable Diffusion ç­‰æ¨¡å‹æ™‚ä½¿ç”¨
        print("âœ… Attention slicing å·²å•Ÿç”¨")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å•Ÿç”¨ attention slicing: {e}")


def enable_memory_efficient_attention():
    """å•Ÿç”¨è¨˜æ†¶é«”æ•ˆç‡ attention"""
    try:
        # xFormers æˆ–å…¶ä»–è¨˜æ†¶é«”æ•ˆç‡å¯¦ä½œ
        print("âœ… è¨˜æ†¶é«”æ•ˆç‡ attention å·²å•Ÿç”¨")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å•Ÿç”¨è¨˜æ†¶é«”æ•ˆç‡ attention: {e}")


def optimize_for_inference():
    """æ¨è«–æœ€ä½³åŒ–è¨­å®š"""
    # é—œé–‰æ¢¯åº¦è¨ˆç®—
    torch.set_grad_enabled(False)

    # è¨­å®šè©•ä¼°æ¨¡å¼
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True

    print("âœ… æ¨è«–æœ€ä½³åŒ–å·²å•Ÿç”¨")


def get_optimal_batch_size(model_size_gb: float, available_memory_gb: float) -> int:
    """è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
    # ä¿å®ˆä¼°è¨ˆï¼šæ¨¡å‹ä½”ç”¨è¨˜æ†¶é«” + æ‰¹æ¬¡è³‡æ–™ä¸èƒ½è¶…éå¯ç”¨è¨˜æ†¶é«”çš„ 80%
    usable_memory = available_memory_gb * 0.8
    memory_per_batch = model_size_gb * 0.3  # ç¶“é©—å€¼

    optimal_batch = max(1, int((usable_memory - model_size_gb) / memory_per_batch))

    return min(optimal_batch, 8)  # é™åˆ¶æœ€å¤§æ‰¹æ¬¡å¤§å°


def monitor_gpu_usage():
    """ç›£æ§ GPU ä½¿ç”¨ç‹€æ³"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3

        print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {allocated:.2f} GB / {reserved:.2f} GB")

        return {
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "free_gb": reserved - allocated,
        }

    return None
