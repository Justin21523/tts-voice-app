# backend/core/model_manager.py
"""
æ¨¡å‹è¼‰å…¥èˆ‡ç®¡ç†
"""

import torch
from typing import Dict, Any, Optional
from pathlib import Path

from backend.core.config import Settings
from backend.core.performance import setup_performance_defaults


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.loaded_models: Dict[str, Any] = {}

        # è¨­å®šæ•ˆèƒ½é è¨­å€¼
        setup_performance_defaults(settings)

        print(f"ğŸ›ï¸ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–")
        print(f"   TTS å¼•æ“: {settings.TTS_ENGINE}")
        print(f"   VC å¼•æ“: {settings.VC_ENGINE}")
        print(f"   è£ç½®: {settings.DEVICE}")
        print(f"   FP16: {settings.FP16}")

    async def load_tts_model(self) -> Any:
        """è¼‰å…¥ TTS æ¨¡å‹"""
        model_key = f"tts_{self.settings.TTS_ENGINE}"

        if model_key in self.loaded_models:
            print(f"â™»ï¸ é‡ç”¨å·²è¼‰å…¥çš„ TTS æ¨¡å‹: {self.settings.TTS_ENGINE}")
            return self.loaded_models[model_key]

        print(f"ğŸ“¥ è¼‰å…¥ TTS æ¨¡å‹: {self.settings.TTS_ENGINE}")

        try:
            if self.settings.TTS_ENGINE == "xtts":
                model = await self._load_xtts_model()
            elif self.settings.TTS_ENGINE == "openvoice":
                model = await self._load_openvoice_model()
            elif self.settings.TTS_ENGINE == "bark":
                model = await self._load_bark_model()
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„ TTS å¼•æ“: {self.settings.TTS_ENGINE}")

            self.loaded_models[model_key] = model
            print(f"âœ… TTS æ¨¡å‹è¼‰å…¥æˆåŠŸ: {self.settings.TTS_ENGINE}")
            return model

        except Exception as e:
            print(f"âŒ TTS æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

    async def load_vc_model(self) -> Any:
        """è¼‰å…¥ VC æ¨¡å‹"""
        model_key = f"vc_{self.settings.VC_ENGINE}"

        if model_key in self.loaded_models:
            print(f"â™»ï¸ é‡ç”¨å·²è¼‰å…¥çš„ VC æ¨¡å‹: {self.settings.VC_ENGINE}")
            return self.loaded_models[model_key]

        print(f"ğŸ“¥ è¼‰å…¥ VC æ¨¡å‹: {self.settings.VC_ENGINE}")

        try:
            if self.settings.VC_ENGINE == "rvc":
                model = await self._load_rvc_model()
            elif self.settings.VC_ENGINE == "sovits":
                model = await self._load_sovits_model()
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„ VC å¼•æ“: {self.settings.VC_ENGINE}")

            self.loaded_models[model_key] = model
            print(f"âœ… VC æ¨¡å‹è¼‰å…¥æˆåŠŸ: {self.settings.VC_ENGINE}")
            return model

        except Exception as e:
            print(f"âŒ VC æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

    async def _load_xtts_model(self):
        """è¼‰å…¥ XTTS æ¨¡å‹ (å¯¦ä½œä½”ä½ç¬¦)"""
        # é€™è£¡æ‡‰è©²å¯¦ä½œå¯¦éš›çš„ XTTS æ¨¡å‹è¼‰å…¥
        # æš«æ™‚å›å‚³æ¨¡æ“¬ç‰©ä»¶
        return {
            "type": "xtts",
            "device": self.settings.DEVICE,
            "fp16": self.settings.FP16,
            "model_path": f"{self.settings.MODELS_TTS_DIR}/xtts",
            "loaded": True,
        }

    async def _load_openvoice_model(self):
        """è¼‰å…¥ OpenVoice æ¨¡å‹ (å¯¦ä½œä½”ä½ç¬¦)"""
        return {
            "type": "openvoice",
            "device": self.settings.DEVICE,
            "fp16": self.settings.FP16,
            "model_path": f"{self.settings.MODELS_TTS_DIR}/openvoice",
            "loaded": True,
        }

    async def _load_bark_model(self):
        """è¼‰å…¥ Bark æ¨¡å‹ (å¯¦ä½œä½”ä½ç¬¦)"""
        return {
            "type": "bark",
            "device": self.settings.DEVICE,
            "fp16": self.settings.FP16,
            "model_path": f"{self.settings.MODELS_TTS_DIR}/bark",
            "loaded": True,
        }

    async def _load_rvc_model(self):
        """è¼‰å…¥ RVC æ¨¡å‹ (å¯¦ä½œä½”ä½ç¬¦)"""
        return {
            "type": "rvc",
            "device": self.settings.DEVICE,
            "fp16": self.settings.FP16,
            "model_path": f"{self.settings.MODELS_VC_DIR}/rvc",
            "loaded": True,
        }

    async def _load_sovits_model(self):
        """è¼‰å…¥ So-VITS-SVC æ¨¡å‹ (å¯¦ä½œä½”ä½ç¬¦)"""
        return {
            "type": "sovits",
            "device": self.settings.DEVICE,
            "fp16": self.settings.FP16,
            "model_path": f"{self.settings.MODELS_VC_DIR}/sovits",
            "loaded": True,
        }

    def unload_model(self, model_key: str):
        """å¸è¼‰æ¨¡å‹"""
        if model_key in self.loaded_models:
            del self.loaded_models[model_key]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"ğŸ—‘ï¸ å·²å¸è¼‰æ¨¡å‹: {model_key}")

    def unload_all_models(self):
        """å¸è¼‰æ‰€æœ‰æ¨¡å‹"""
        self.loaded_models.clear()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print(f"ğŸ—‘ï¸ å·²å¸è¼‰æ‰€æœ‰æ¨¡å‹")

    def is_tts_loaded(self) -> bool:
        """æª¢æŸ¥ TTS æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥"""
        return f"tts_{self.settings.TTS_ENGINE}" in self.loaded_models

    def is_vc_loaded(self) -> bool:
        """æª¢æŸ¥ VC æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥"""
        return f"vc_{self.settings.VC_ENGINE}" in self.loaded_models

    def get_loaded_models(self) -> Dict[str, str]:
        """å–å¾—å·²è¼‰å…¥æ¨¡å‹æ¸…å–®"""
        return {k: v.get("type", "unknown") for k, v in self.loaded_models.items()}

    def get_memory_usage(self) -> Dict[str, Any]:
        """å–å¾—è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³"""
        if torch.cuda.is_available():
            return {
                "gpu_allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
                "gpu_reserved": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB",
                "gpu_free": f"{torch.cuda.memory_reserved() - torch.cuda.memory_allocated() / 1024**3:.2f} GB",
            }
        else:
            return {"gpu_available": False}
