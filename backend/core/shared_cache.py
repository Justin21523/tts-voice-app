# backend/core/shared_cache.py
"""
å…±äº«å¿«å–åˆå§‹åŒ–
"""

import os
import torch
from pathlib import Path
from typing import Dict


def setup_shared_cache(cache_root: str = None) -> Dict[str, str]:  # type: ignore
    """
    è¨­å®šå…±äº« AI æ¨¡å‹å¿«å–

    Args:
        cache_root: å¿«å–æ ¹ç›®éŒ„ï¼Œé è¨­å¾ç’°å¢ƒè®Šæ•¸æˆ– ../ai_warehouse/cache

    Returns:
        è¨­å®šçš„ç’°å¢ƒè®Šæ•¸å­—å…¸
    """
    if cache_root is None:
        cache_root = os.getenv("AI_CACHE_ROOT", "../ai_warehouse/cache")

    # ç¢ºä¿å¿«å–æ ¹ç›®éŒ„å­˜åœ¨
    Path(cache_root).mkdir(parents=True, exist_ok=True)

    # HuggingFace å¿«å–è¨­å®š
    hf_cache_vars = {
        "HF_HOME": f"{cache_root}/hf",
        "TRANSFORMERS_CACHE": f"{cache_root}/hf/transformers",
        "HF_DATASETS_CACHE": f"{cache_root}/hf/datasets",
        "HUGGINGFACE_HUB_CACHE": f"{cache_root}/hf/hub",
    }

    # PyTorch å¿«å–è¨­å®š
    torch_cache_vars = {
        "TORCH_HOME": f"{cache_root}/torch",
        "TORCH_EXTENSIONS_DIR": f"{cache_root}/torch/extensions",
    }

    # æ‡‰ç”¨ç‰¹å®šç›®éŒ„
    app_dirs = {
        "MODELS_TTS_DIR": f"{cache_root}/models/tts",
        "MODELS_VC_DIR": f"{cache_root}/models/vc",
        "MODELS_ASR_DIR": f"{cache_root}/models/asr",
        "VOICE_SPEAKERS_DIR": f"{cache_root}/voice/speakers",
        "VOICE_OUTPUTS_DIR": f"{cache_root}/outputs/voice-app",
    }

    # åˆä½µæ‰€æœ‰è¨­å®š
    all_cache_vars = {**hf_cache_vars, **torch_cache_vars, **app_dirs}

    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¸¦å»ºç«‹ç›®éŒ„
    for key, path in all_cache_vars.items():
        os.environ[key] = path
        Path(path).mkdir(parents=True, exist_ok=True)

    # é¡¯ç¤ºè¨­å®šè³‡è¨Š
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0

    print(f"ğŸ  å…±äº«å¿«å–æ ¹ç›®éŒ„: {cache_root}")
    print(f"ğŸ”¥ GPU å¯ç”¨: {gpu_available} ({gpu_count} è£ç½®)")

    if gpu_available:
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {gpu_memory:.1f} GB")

    return all_cache_vars


def get_cache_info() -> Dict[str, str]:
    """å–å¾—ç•¶å‰å¿«å–è¨­å®šè³‡è¨Š"""
    cache_vars = [
        "AI_CACHE_ROOT",
        "HF_HOME",
        "TRANSFORMERS_CACHE",
        "TORCH_HOME",
        "MODELS_TTS_DIR",
        "MODELS_VC_DIR",
    ]

    return {var: os.getenv(var, "æœªè¨­å®š") for var in cache_vars}


def cleanup_cache(cache_root: str = None, keep_models: bool = True):  # type: ignore
    """
    æ¸…ç†å¿«å–ç›®éŒ„

    Args:
        cache_root: å¿«å–æ ¹ç›®éŒ„
        keep_models: æ˜¯å¦ä¿ç•™æ¨¡å‹æª”æ¡ˆ
    """
    if cache_root is None:
        cache_root = os.getenv("AI_CACHE_ROOT", "/tmp/ai_cache")

    import shutil

    cleanup_dirs = [
        f"{cache_root}/outputs",
        f"{cache_root}/temp",
    ]

    if not keep_models:
        cleanup_dirs.extend(
            [
                f"{cache_root}/hf/transformers",
                f"{cache_root}/torch",
            ]
        )

    for dir_path in cleanup_dirs:
        path = Path(dir_path)
        if path.exists():
            shutil.rmtree(path)
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†: {dir_path}")

    print(f"âœ… å¿«å–æ¸…ç†å®Œæˆ")
